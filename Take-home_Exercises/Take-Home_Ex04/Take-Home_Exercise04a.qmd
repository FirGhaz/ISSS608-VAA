---
title: "Big Mac Index Choropleth and Predictive Model"
subtitle: "Take Home Exercise 04"
author: "FirGhaz"
date: 02/03/2024
date modified: last modified
execute:
  eval: true
  message: false
  echo: true
  warning: false
  format:
  html:
    code-fold: true
    code-summary: "code block"
    code-tools: false
    code-copy: true
---

# 1 Load Packages and Data

```{r}
if (!require("tidyverse")) install.packages("tidyverse", dependencies = TRUE)
if (!require("sf")) install.packages("sf", dependencies = TRUE)
if (!require("rnaturalearth")) install.packages("rnaturalearth", dependencies = TRUE)
if (!require("countrycode")) install.packages("countrycode", dependencies = TRUE)
if (!require("ggrepel")) install.packages("ggrepel", dependencies = TRUE)
```

```{r}
library("tidyverse")
library("sf")
library("rnaturalearth")
library("countrycode")
library("ggrepel")
library(plotly)
```

Get World data

```{r}
worldnew <- ne_countries(scale="small", returnclass = "sf")
```

```{r}
worldnew %>%
  ggplot() + 
  geom_sf()
```

Change map projection and coordinate reference system

```{r}
worldnew %>%
  #can use moll, aeqd, goode, robin 
  #projection website: https://proj.org/operations/projections/
  st_transform(crs = "+proj=robin") %>% 
  ggplot() +
  geom_sf() +
  #coord_sf(datum = NA) +
  theme_minimal()
```

Read the dataset

```{r}
bmi_data_raw <- read_csv("data/bmi_data.csv")
```

adding in ISO3 Country Code

```{r}
bmi_data_iso <- bmi_data_raw%>%
  mutate (Iso3 = countrycode :: countrycode(
    sourcevar = country,
    origin = "country.name",
    destination = "iso3c")
  )
```

Join datasets

```{r}
world_bmi <- worldnew %>%
  dplyr::select (geometry, name, iso_a3) %>%
  left_join(bmi_data_iso, by = c("iso_a3" = "Iso3")) # %>%
  #filter(year == 2020)

world_bmi
```

```{r}
library(ggplot2)
library(sf)
library(dplyr)

world_bmi2 <- worldnew %>% 
  filter(admin != "Antarctica") %>%
  st_transform(crs ="+proj=robin") %>%
  ggplot() +
  geom_sf(color = "grey") +  # Fixed by adding a '+' at the end of this line
  geom_sf(data = world_bmi, aes(fill = bmi_usd_price)) +  # Removed quotes around bmi_usd_price
  scale_fill_viridis_c() +  # Changed to a continuous color scale
  theme_minimal() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_blank(),
        legend.position = "bottom") +  # Changed legend position to bottom
  labs(title = "BMI Price (USD)",
       subtitle = "Countries BMI Index at 2020",
       x = NULL, y = NULL, caption = "Made by Firdaus")

print(world_bmi2)
```

by plotly

```{r}
fontStyle = list(
  family = "DM Sans",
  size = 15,
  color = "black"
)

label = list(
  bgcolor = "#EEEEEE",
  bordercolor = "transparent",
  font = fontStyle
)
```

```{r}
bmi_geo <- plot_ly(data = world_bmi, 
                   locationmode= 'ISO-3', 
                   frame = ~year) %>%
  add_trace(locations = ~iso_a3,
            z = ~bmi_usd_price,
            zmin = 0,
            zmax = 8,
            type = 'choropleth',
            text = ~name,
            #("Country: ", name, "<br>"
                                 #"BMI USD Price: $", sprintf("%.2f", bmi_usd_price), "<br>",
                                 #"GDP per Capita: $", sprintf("%.2f", gdp_per_capita), "<br>",
                                 #"Inflation: ", sprintf("%.2f%%", inflation)), 
            colorscale = "Electric", 
            color = ~bmi_usd_price) %>%
  layout(geo = list(projection = list(type = 'natural earth'),
          font = list(family = "DM Sans"),
          title = 'Global Big Mac Index (USD Price) from 2002 to 2021')) %>%
  style(hoverlabel = label) %>%
  config (displayModeBar = FALSE) %>%
  colorbar(tickprefix = "$")

bmi_geo


```

#1.5 Network Choropleth

```{r}
#install.packages("tidygraph")
library(tidygraph)
library(igraph)
library(ggplot2)
library(ggraph)

# Load the datasets
import_export_beef <- read.csv('C:/FirGhaz/ISSS608-VAA/Take-home_Exercises/Take-Home_Ex04/data/import_export_beef_v5.csv')
bmi_data <- read.csv('data/bmi_data.csv')

# Preview the datasets
head(import_export_beef)
head(bmi_data)

```

```{r}
library(dplyr)
library(igraph)

# Assuming 'country_name' is the common key and 'ISO3' is the column you want to join from 'world_bmi' to 'import_export_beef'
import_export_beef_enriched <- import_export_beef %>%
  left_join(world_bmi, by = "country") %>% 
  select(-matches("^world_bmi"))  # Remove duplicated columns if needed

# Filter the dataset for a specific year
year_2021_trade <- import_export_beef_enriched %>%
 filter(Year == 2021) #!is.na(country)) #!is.na(Partner))  # Adjust the year as needed

#Create an igraph object from the filtered data
trade_network <- graph_from_data_frame(d = year_2021_trade, directed = TRUE)

```

```{r}
# Basic network plot with ggraph
library(ggraph)

# Visualize the network
ggraph(trade_network, layout = "graphopt") + 
  geom_edge_link(aes(color = Trade, edge_WIDTH = Value), alpha = 0.05) + # Use 'element' for color
  scale_edge_color_manual(values = c("Import Quantity" = "#00CCB0", "Export Quantity" = "#FF6ABE")) + # Customize colors
  geom_node_point(color='red', size = 1) +
  geom_node_text(aes(label = name), repel = FALSE, size = 2, color = "white") + 
  theme_minimal() +
  ggtitle("Beef Imports | Exports Network 2021") +
  theme_graph(background = 'grey10',
                text_colour = 'white')
```


```{r}
# Basic network plot with ggraph
library(ggraph)

# Visualize the network
ggraph(trade_network, layout = "linear", circular = TRUE) + 
  geom_edge_arc(aes(color = Trade, edge_WIDTH = Value), alpha = 0.05) + # Use 'element' for color
  scale_edge_color_manual(values = c("Import Quantity" = "#00CCB0", "Export Quantity" = "#FF6ABE")) + # Customize colors
  geom_node_point(color = "red", size = 1) +
  geom_node_text(aes(label = name), repel = FALSE, size = 2, color = "white") + 
  theme_minimal() +
  ggtitle("Beef Imports | Exports Network 2021") + coord_fixed() +
  theme_graph(background = 'grey10',
                text_colour = 'white')
```
```{r}
bmi_node_2021 <- read.csv('C:/FirGhaz/ISSS608-VAA/Take-home_Exercises/Take-Home_Ex04/data/bmi_node_trial.csv')
bmi_edges_2021 <- read.csv('C:/FirGhaz/ISSS608-VAA/Take-home_Exercises/Take-Home_Ex04/data/import_export_beef_edges5-2021_v2_trial.csv')
```

```{r}
bmi_edges_2021 <- na.omit(bmi_edges_2021)
bmi_edges_2021
```



```{r}
bmi2021_graph <- tbl_graph(nodes = bmi_node_2021,
                           edges = bmi_edges_2021, 
                           directed = TRUE)
```


```{r}
bmi2021_graph
```
```{r}
ggraph(bmi2021_graph) +
  geom_edge_link() +
  geom_node_point()
g + theme_graph(background = 'grey10',
                text_colour = 'white')
```
```{r}
g <- ggraph(bmi2021_graph, 
            layout = "fr") + 
  geom_edge_link(aes(color = Trade, width = Value), alpha = 0.8) + scale_edge_color_manual(values = c("Import Quantity" = "#00CCB0", "Export Quantity" = "#FF6ABE")) +
  scale_edge_width(range = c(0.1,5)) +
  geom_node_point(aes(colour = country, 
                      size = 0.7)) +
  theme_graph() +
  guides(size = "none", 
         edge_alpha = "none",
         Value = "none") 
g
```
```{r}
set_graph_style()

g <- ggraph(bmi2021_graph, 
            layout = "fr") + 
  geom_edge_link(aes(width=Value), 
                 alpha=0.2) +
  scale_edge_width(range = c(0.1, 5)) +
  geom_node_point(aes(colour = continent), 
                  size = 2) +
  theme(legend.position = 'bottom',
        legend.text = element_text(size = 5))  # Adjust legend text size
        
g + facet_nodes(~continent)+
th_foreground(foreground = "grey80",  
                border = TRUE) 
  
```
Computing Network Centrality Indices

```{r}
g <- bmi2021_graph %>%
  mutate(betweenness_centrality = centrality_betweenness()) %>%
  ggraph(layout = "fr") + 
  geom_edge_link(aes(width=Value), 
                 alpha=0.5) +
  scale_edge_width(range = c(0.1, 5)) +
  geom_node_point(aes(colour = country,
            size=betweenness_centrality))
g + theme_graph()
```
**Degree Centrality**
Degree centrality measures the number of edges connected to a node. In directed networks, you can distinguish between in-degree and out-degree.

Degree centrality is a measure used in network analysis to quantify the importance or influence of a particular node within a network. It is based on the number of connections, or edges, that a node has to other nodes. The central concept behind degree centrality is simple: nodes with more connections are considered more central and potentially more influential within the network.

There are two types of degree centrality:

In-Degree Centrality: This measures the number of incoming connections to a node. It can be particularly relevant in directed networks where the direction of the connection matters. A high in-degree centrality indicates that a node is a major target within the network, receiving many connections from other nodes. This can signify a node of high interest or popularity.

Out-Degree Centrality: This measures the number of outgoing connections from a node. Like in-degree centrality, it is applicable in directed networks. A high out-degree centrality signifies that a node actively reaches out to many other nodes, which can indicate a source or distributor of information, goods, or influence within the network


```{r}
# First, calculate the degree centrality for each node in the graph
degree_centrality <- degree(bmi2021_graph, mode = "all") # 'all' for undirected graphs

# Add the degree centrality as an attribute to the nodes in the graph
V(bmi2021_graph)$degree_centrality <- degree_centrality

# Now, use ggraph to visualize the network
g <- ggraph(bmi2021_graph, layout = "fr") + 
  geom_edge_link(aes(width = Value), alpha = 0.5) +
  scale_edge_width(range = c(0.1, 5)) +
  geom_node_point(aes(colour = country, size = degree_centrality), alpha = 0.7) + # Use degree_centrality here
  theme_graph()

# Optionally, adjust theme settings
g <- g + theme(legend.position = 'bottom')
 print(g)
```
Degree centrality is a straightforward but powerful concept in network analysis, useful for identifying key nodes that might play critical roles in the dissemination of information, disease transmission, social network influence, and more within a network.

**Eigenvector Centrality**
Eigenvector centrality measures a node's influence based on the principle that connections to high-scoring nodes contribute more to the score of the node in question.

```{r}
library(igraph)
library(ggraph)

# Assuming bmi2021_graph is an igraph object

# Calculate the eigenvector centrality for each node
eigenvector_centrality <- eigen_centrality(bmi2021_graph)$vector

# Add the eigenvector centrality as an attribute to the nodes in the graph
V(bmi2021_graph)$eigenvector_centrality <- eigenvector_centrality

# Now, use ggraph to visualize the network
g <- ggraph(bmi2021_graph, layout = "fr") + 
  geom_edge_link(aes(width = Value), alpha = 0.5) +
  scale_edge_width(range = c(0.1, 5)) +
  geom_node_point(aes(colour = country, size = eigenvector_centrality), alpha = 0.7) + # Use eigenvector_centrality here
  theme_graph()

# Optionally, adjust theme settings
g <- g + theme(legend.position = 'bottom')

# Print the graph
print(g)

```


eigenvector centrality is useful for identifying influential nodes in a network where connections to high-scoring nodes contribute more to the score of the node than equal connections to low-scoring nodes.

**Closeness centrality**

```{r}
library(igraph)

# Assuming bmi2021_graph is your igraph graph object

# Calculate closeness centrality
closeness_centrality <- closeness(bmi2021_graph, mode="all")

# Add the closeness centrality as an attribute to the nodes
V(bmi2021_graph)$closeness_centrality <- closeness_centrality

# To visualize with ggraph (if your graph is large/complex):
library(ggraph)
set.seed(123) # For reproducible layout

ggraph(bmi2021_graph, layout = 'fr') +
  geom_edge_link(color = 'gray80', alpha = 0.5) +
  geom_node_point(aes(colour = country, size = closeness_centrality)) +
  scale_size(range = c(1, 5)) + # Adjust according to your preference
  theme_graph() +
  ggtitle("BMI2021 Network (Nodes Sized by Closeness Centrality)")

# If you simply want to plot using igraph's plot function:
plot(bmi2021_graph, vertex.size = V(bmi2021_graph)$closeness_centrality * 300,
     main = "BMI2021 Network (Nodes Sized by Closeness Centrality)",
     vertex.label = NA, vertex.color = "steelblue")

```


Putting the results in a dataframe.

```{r}
library(igraph)
library(dplyr)

degree_centrality <- degree(bmi2021_graph)
betweenness_centrality <- betweenness(bmi2021_graph)
closeness_centrality <- closeness(bmi2021_graph)
eigenvector_centrality <- eigen_centrality(bmi2021_graph)$vector

centrality_measures <- data.frame(
  node = V(bmi2021_graph)$country,  # or use V(bmi2021_graph)$name if nodes are named
  degree = degree_centrality,
  betweenness = betweenness_centrality,
  closeness = closeness_centrality,
  eigenvector = eigenvector_centrality
)

print(centrality_measures)


centrality_measures_arranged <- centrality_measures %>%
  arrange(desc(degree)) 

# View the arranged dataframe
print(centrality_measures_arranged)

```


```{r}
library(igraph)
library(dplyr)
library(cluster)  # For clustering
library(ggplot2)  # For visualization

library(dplyr)
library(cluster)  # For k-means clustering
library(ggplot2)  # For visualization

# Assuming your dataframe is named centrality_measures and has the structure as you described
centrality_measures <- data.frame(
  node = c("United States", "Brazil", "Australia", "United Kingdom", "New Zealand", "Canada", "Japan", "Denmark", "Argentina", "Hong Kong", "South Africa", "China", "Poland", "Singapore", "Mexico", "Philippines", "Thailand", "Chile", "Malaysia", "Indonesia", "Switzerland", "Hungary", "Czech Rep.", "Russia", "Peru", "Korea", "Sweden", "Turkey"),
  degree = c(46, 42, 41, 36, 35, 34, 28, 27, 26, 26, 25, 23, 23, 18, 16, 16, 15, 14, 14, 13, 10, 9, 8, 8, 7, 7, 7, 2),
  betweenness = c(63.02704678, 105.65333742, 39.36949191, 40.09939147, 56.87390826, 17.32899958, 23.02480206, 41.06001747, 43.40572691, 25.60488769, 7.19925192, 32.78891832, 28.37051720, 3.41626984, 0.30277778, 2.15634921, 1.39365079, 0.90476190, 2.20634921, 0.82976190, 3.50753968, 3.18060777, 0.0, 0.125, 0.0, 0.0, 1.11507937, 0.05555556),
  closeness = c(0.02702703, 0.02941176, 0.02702703, 0.02564103, 0.025, 0.02380952, 0.02272727, 0.02222222, 0.02222222, 0.02564103, 0.02173913, 0.02272727, 0.02, 0.02127660, 0.01851852, 0.02173913, 0.02, 0.02, 0.02083333, 0.02, 0.02040816, 0.01724138, 0.01694915, 0.01960784, 0.01851852, 0.0, 0.02, 0.01562500),
  eigenvector = c(1.00000000, 0.79216462, 0.91131031, 0.82043704, 0.75869650, 0.85520898, 0.68979082, 0.51206887, 0.52344707, 0.62049683, 0.66952721, 0.53404899, 0.33534600, 0.46351272, 0.48718072, 0.46476065, 0.41084699, 0.39923377, 0.32964467, 0.35498179, 0.27335857, 0.13210958, 0.12324034, 0.18349413, 0.19506041, 0.17241749, 0.15801139, 0.05760238)
)

# Scale the data
data_for_clustering <- scale(centrality_measures[,-1])  # Exclude the 'node' column

# Perform k-means clustering
set.seed(123)  # For reproducibility
k <- 5  # Number of clusters
clustering_result <- kmeans(data_for_clustering, centers = k, nstart = 25)

# Add the cluster assignment to the original dataframe
centrality_measures$cluster <- clustering_result$cluster

# Print the clustering result
print(table(centrality_measures$cluster))

# Visualizing the clusters based on the first two principal components
pca_result <- prcomp(data_for_clustering)
centrality_measures$pca1 <- pca_result$x[, 1]
centrality_measures$pca2 <- pca_result$x[, 2]

ggplot(centrality_measures, aes(x = pca1, y = pca2, color = as.factor(cluster))) +
  geom_point() +
  theme_minimal() +
  labs(color = "Cluster", title = "Cluster Analysis of Countries based on Centrality Measures",
       x = "Principal Component 1", y = "Principal Component 2")


```

```{r}
library(plotly)

# Assuming centrality_measures dataframe from the previous step is used
plot_ly(centrality_measures, x = ~pca1, y = ~pca2, type = 'scatter', mode = 'markers',
        hoverinfo = 'text',
        text = ~paste('Country: ', node, 
                      '<br>degree: ', round(degree, 2), 
                      '<br>betweenness: ', round(betweenness, 2),
                      '<br>closeness: ', round(closeness, 2),
                      '<br>eigenvector: ', round(eigenvector, 2),
                      '<br>Cluster: ', centrality_measures$cluster),
        marker = list(size = 10, 
                      color = as.numeric(centrality_measures$cluster),  # Color by cluster
                      colorscale = 'Viridis',  # You can choose other color scales
                      line = list(color = 'black', width = 1))) %>%
  layout(title = 'Cluster Analysis of Countries based on Centrality Measures',
         xaxis = list(title = 'Principal Component 1'),
         yaxis = list(title = 'Principal Component 2'),
         hovermode = 'closest')

```


**Visualising Community**

Tidygraph package inherits many of the community detection algorithms imbedded into igraph We will utilise 3 community detection algo: (1) Edge-betweenness (group_edge_betweenness), (2) Leading eigenvector (group_leading_eigen), (3) Fast-greedy (group_fast_greedy) and (4) Spinglass (group_spinglass).Some community algorithms are designed to take into account direction or weight, while others ignore it.

**group_edge_betweeness**
```{r}
g <- bmi2021_graph %>%
  mutate(community = as.factor(group_edge_betweenness(weights = Value, directed = TRUE))) %>%
  ggraph(layout = "fr") + 
  geom_edge_link(aes(width=Value), 
                 alpha=0.2) +
  scale_edge_width(range = c(0.1, 5)) +
  geom_node_point(aes(colour = community))  

g + theme_graph()
```

**group_leading Eigenvector**
```{r}
g <- bmi2021_graph %>%
  mutate(community = as.factor(group_leading_eigen(weights = Value))) %>%
  ggraph(layout = "fr") + 
  geom_edge_link(aes(width = Value), alpha = 0.2) +
  scale_edge_width(range = c(0.1, 5)) +
  geom_node_point(aes(colour = community)) +
  theme_graph()

g
```


**WalkTrap**
The Walktrap algorithm is another method for detecting communities in graphs. It attempts to find densely connected subgraphs (communities) in a graph based on random walks. The idea is that short random walks tend to stay in the same community.
```{r}
library(tidygraph)
library(ggraph)

# Ensure bmi2021_graph is a tbl_graph; if it's an igraph object, convert it
bmi2021_graph_tbl <- as_tbl_graph(bmi2021_graph)

g <- bmi2021_graph_tbl %>%
  mutate(community = as.factor(group_walktrap())) %>%
  ggraph(layout = "fr") + 
  geom_edge_link(aes(width = Value), alpha = 0.2) +
  scale_edge_width(range = c(0.1, 5)) +
  geom_node_point(aes(colour = community)) +
  theme_graph()

g

```

**Spinglass**
```{r}
g <- bmi2021_graph %>%
  mutate(community = as.factor(group_spinglass(weights = Value))) %>%
  ggraph(layout = "fr") + 
  geom_edge_link(aes(width = Value), alpha = 0.2) +
  scale_edge_width(range = c(0.1, 5)) +
  geom_node_point(aes(colour = community)) +
  theme_graph()

g
```


**Interactivity**

```{r}
library(visNetwork)
bmi_edges_2021_aggregated <- bmi_edges_2021 %>%
  left_join(bmi_node_2021, by = c("Origin" = "country")) %>%
  rename(from = id) %>%
  left_join(bmi_node_2021, by = c("Partners" = "country")) %>%
  rename(to = id) %>%
   group_by(from, to) %>%
    summarise(Value = n()) %>%
 filter(from!=to) %>%
  #filter(Value > 1) %>%
  ungroup()

```

```{r}
bmi_node_2021a <- bmi_node_2021 %>%
  rename(group = continent) 
```



```{r}
visNetwork(bmi_node_2021a,
           bmi_edges_2021_aggregated) %>%
  visIgraphLayout(layout = "layout_with_kk") %>%
  visOptions(highlightNearest = TRUE,
             nodesIdSelection = TRUE) %>%
  visEdges(arrows = "to", 
          smooth = list(enabled = TRUE, 
                         type = "curvedCW")) %>%
  visLegend() %>%
  visLayout(randomSeed = 123)
```

















```{r}
library(sf)

world_borders <- st_read("C:/FirGhaz/ISSS608-VAA/Take-home_Exercises/Take-Home_Ex04/data/geospatial/TM_WORLD_BORDERS-0.3.shp")
```

```{r}
library(sf)

# Assuming 'world_borders' is already loaded with st_read()

# Correct geometry issues
world_borders_valid <- st_make_valid(world_borders)

# Calculate centroids for corrected geometries
world_centroids <- st_centroid(world_borders_valid)

# Extract latitude and longitude
world_centroids <- world_centroids %>% 
  mutate(longitude = st_coordinates(.)[, 1],
         latitude = st_coordinates(.)[, 2])
```

```{r}
# Select necessary columns (assuming ISO3 is used as the identifier)
geo_data <- world_centroids %>% 
  select(ISO3, latitude, longitude)


```

```{r}
library(dplyr)

# Loop through each node by index
for(i in seq_along(V(trade_network)$name)) {
  # Find the corresponding ISO3 code for each node based on 'country'
  node_name <- V(trade_network)$name[i]
  iso_code <- import_export_beef_enriched %>%
    filter(country == node_name) %>%
    pull(ISO3) %>%
    unique()  # Ensuring unique ISO3 code is selected in case of multiple matches
  
  #Check if ISO3 code is found and assign it
  if(length(iso_code) == 1) {
   V(trade_network)$ISO3[i] <- iso_code
  } else {
   V(trade_network)$ISO3[i] <- NA  # Assign NA if no matching ISO3 code is found
  }
}

```

```{r}
# Assuming your igraph object 'trade_network' has node names that match country codes
network_nodes <- data.frame(
  name = V(trade_network)$name,
  ISO3 = V(trade_network)$ISO3)
```

```{r}
# Merge latitude and longitude with network nodes
network_nodes_geo <- merge(network_nodes, geo_data, by = "ISO3")

```

```{r}
# Assuming 'network_nodes_geo' contains 'ISO3', 'latitude', and 'longitude'
# and 'edges_df' has 'from' and 'to' as ISO3 codes
edges_df <- get.data.frame(trade_network, what = "edges")
# Ensure ISO3 codes are used in edges_df (if necessary, adjust this step based on your data)
edges_df$from <- V(trade_network)$ISO3[match(edges_df$from, V(trade_network)$name)]
edges_df$to <- V(trade_network)$ISO3[match(edges_df$to, V(trade_network)$name)]

# Merge geographic coordinates for both 'from' and 'to'
edges_geo <- edges_df %>%
  left_join(network_nodes_geo, by = c("from" = "ISO3")) %>%
  rename(from_lat = latitude, from_long = longitude) %>%
  left_join(network_nodes_geo, by = c("to" = "ISO3")) %>%
  rename(to_lat = latitude, to_long = longitude)

head(edges_geo)


```


```{r}
#net_geo_graph <- tbl_graph(nodes = network_nodes_geo,
                           #edges = edges_geo, 
                           #directed = TRUE)
```

```{r}
#write.csv(network_nodes_geo, "C:/FirGhaz/ISSS608-VAA/Take-home_Exercises/Take-Home_Ex04/network_nodes_geo.csv", row.names = FALSE)
```

```{r}
#write.csv(edges_geo, "C:/FirGhaz/ISSS608-VAA/Take-home_Exercises/Take-Home_Ex04/edges_geo.csv", row.names = FALSE)
```
























```{r}
library(ggplot2)

edges_geo <- edges_geo %>%
  mutate(alpha = ifelse(Trade == "Export Quantity", 0.5, 0.5)) 

ggplot() + 
  geom_sf(data = worldnew , fill = "#1A1A59", color = "#8080B3", line = "#8080B3") + 
  geom_segment(data = edges_geo, aes(x = from_long, y = from_lat, xend = to_long, yend = to_lat, color = Trade), size = 0.001, alpha = 0.5) + 
  scale_color_manual(values = c("Import Quantity" = "#53C9CF", "Export Quantity" = "#FF6ABE")) +
  geom_point(data = network_nodes_geo, aes(x = longitude, y = latitude), color = "orange", size = 1) +
  theme_dark()
```

```{r}
# Assuming 'edges_geo' has 'from', 'to', 'Element', and 'Value' columns where 'Value' is the trade volume
exports <- edges_geo %>% 
  filter(Trade == "Export Quantity") %>%
  group_by(from) %>%
  summarise(Total_Export = sum(Value))

imports <- edges_geo %>%
  filter(Trade == "Import Quantity") %>%
  group_by(to) %>%
  summarise(Total_Import = sum(Value))

# Merge exports and imports and calculate net export
net_exports <- merge(exports, imports, by.x = "from", by.y = "to", all = TRUE) %>%
  mutate(Total_Export = ifelse(is.na(Total_Export), 0, Total_Export),
         Total_Import = ifelse(is.na(Total_Import), 0, Total_Import),
         Net_Export = Total_Export - Total_Import) %>%
  select(from, Net_Export)

# Rename 'from' to 'ISO3' for joining
colnames(net_exports)[1] <- "ISO3"

```

```{r}
network_nodes_geo_exports <- network_nodes_geo %>%
  left_join(net_exports, by = "ISO3") %>%
  mutate(Size = ifelse(is.na(Net_Export), 2, sqrt(abs(Net_Export))))  # Adjust the size calculation as needed

```

```{r}
ggplot() + 
  geom_sf(data = worldnew , fill = "#140A29", color = "#8080B3", line = "#8080B3") + 
  geom_point(data = network_nodes_geo_exports, aes(x = longitude, y = latitude, size = Size), color = "orange", alpha = 0.5) +
  geom_segment(data = edges_geo, aes(x = from_long, y = from_lat, xend = to_long, yend = to_lat, color = Trade), 
               size = 0.5, alpha = 0.02) + 
  scale_color_manual(values = c("Import Quantity" = "#53C9CF", "Export Quantity" = "#FF6ABE")) +
  theme_dark() +
  guides(size = FALSE, color = guide_legend(override.aes = list(size = 2, alpha = 2))) +  
  labs(color = "Exports | Imports (Beef) ") 
```

```{r}
# Determine the range for the sizes in the plot
min_size <- 3
max_size <- 25

size_range <- range(network_nodes_geo_exports$Size, na.rm = TRUE)

scaling_factor <- (max_size - min_size) / (size_range[2] - size_range[1])

scaled_size <- (network_nodes_geo_exports$Size - size_range[1]) * scaling_factor + min_size

```

```{r}
library(plotly)

network_nodes_geo_exports$hover_text <- paste(
  "Country: ", network_nodes_geo_exports$name,
  "<br>Import: ", #edges_geo$export_usd,  # Replace with your actual import column name
  "<br>Export: ", #edges_geo$import_usd,  # Replace with your actual export column name
  "<br>BMI USD Price: ", #edges_geo$bmi_usd_price,
  sep = ""
)

# Create the base map with interactive nodes
p <- plot_geo(worldnew, locationmode = 'ISO-3') %>%
  add_trace(
    type = 'scattergeo',
    locations = network_nodes_geo_exports$ISO3,
    text = network_nodes_geo_exports$hover_text,  # Use the hover text for nodes
    hoverinfo = 'text',
    mode = 'markers',
    marker = list(
      size = scaled_size,  # Make sure this is defined as shown in previous examples
      color = 'yellow',
      opacity = 0.3
    )
  )

# Overlay the trade lines (make them static)
p <- p %>%
  add_segments(
    x = ~edges_geo$from_long,
    y = ~edges_geo$from_lat,
    xend = ~edges_geo$to_long,
    yend = ~edges_geo$to_lat,
    hoverinfo = 'none',
    line = list(
      color = ~ifelse(edges_geo$Element == "Export Quantity", "#FFA054", "#33C9CF"),
      width = 0.007,
      alpha = 0.2
    )
  )

# Customize the layout
p <- p %>%
  layout(
    title = 'World Trade Network (Interactive Nodes)',
    geo = list(
      showland = TRUE,
      landcolor = toRGB("#140A29"),
      countrycolor = toRGB("#8080B3"),
      coastlinecolor = toRGB("#8080B3"),
      projection = list(type = 'natural earth')
    ),
    legend = list(orientation = 'h')
  )

# Display the plot
p
```

```{r}
pacman::p_load(ggrepel, patchwork, 
               ggthemes, hrbrthemes,
               ggdist, ggridges,
               colorspace, gridExtra, treemap, treemapify,
               tidyverse, tmap, sf, tmaptools, dplyr, tibble, dplyr) 
```

Importing data

```{r}
big_mac <- read_csv("data/countries_with_complete_data.csv")
```

# 2 ChoroPleth

```{r}
data("World")
```

```{r}
library(dplyr)

wmpb_World <- World %>% 
  dplyr::select(-HPI, -inequality, -footprint, -life_exp, -well_being, -gdp_cap_est)
wmpb_World
```

```{r}
wmpb_World_BMI <- wmpb_World %>% left_join(big_mac,
                          by = c("name" = "country"))
```

```{r}
library(dplyr)

wmpb_World_BMI_2020 <- wmpb_World_BMI %>%
  filter(year == 2020) %>% 
  group_by(name) %>%
  filter(
    any(!is.na(currency_code)) & 
    any(!is.na(bmi_localprice)) & 
    any(!is.na(bmi_usd_price)) & 
    any(!is.na(export_usd)) & 
    any(!is.na(import_usd)) & 
    any(!is.na(GDP)) & 
    any(!is.na(gdp_per_capita)) & 
    any(!is.na(gdp_per_employed)) & 
    any(!is.na(inflation)) & 
    any(!is.na(year))
  ) %>%
  ungroup()

```

```{r}
tmap_mode("view")

tm <- tm_shape(wmpb_World_BMI_2020) +
  tm_polygons("bmi_usd_price", id = "name", popup.vars = c("Big Mac Index(USD)" = "bmi_usd_price", 
                                                       "Inflation" = "inflation", 
                                                       "Population Density" = "pop_est_dens", 
                                                       "GDP per Employed" = "gdp_per_employed",
                                                       "BMI local Price" = "bmi_localprice",
                                                       "Econonic Status" = "economy",
                                                       "Income" = "income_grp"),
            popup.format = list(pop_est_dens = list(digits = 1), gdp_per_employed = list(digits = 0), inflation = list(digits = 1)
  )) +
  tm_layout()

tm
#qtm(wmpb_World_BMI_2020, 
   #fill = "usd_price")
```

```{r}
tm_shape(wmpb_World_BMI_2020) +
  tm_fill("bmi_usd_price",
          style = "quantile",
          palette = "Blues",
          thres.poly = 0) + 
  tm_facets(by="income_grp", 
            free.coords=TRUE, 
            drop.shapes=FALSE) +
  tm_layout(legend.show = FALSE,
            title.position = c("center", "right"), 
            title.size = 20) +
  tm_borders(alpha = 0.5)
```

```{r}
tm_shape(wmpb_World_BMI_2020) +
  tm_fill("bmi_usd_price",
          style = "quantile",
          palette = "Blues",
          legend.hist = TRUE, 
          legend.is.portrait = TRUE,
          legend.hist.z = 0.1,
          thres.poly = 0) +  
  tm_facets(by="economy", 
            free.coords=TRUE, 
            drop.shapes=FALSE) +
  tm_layout(legend.show = FALSE,
            title.position = c("center", "right"), 
            title.size = 20) +
  tm_borders(alpha = 0.5)
 tm_layout(legend.position = c("bottom", "bottom"))
```

```{r}
var <- wmpb_World_BMI_2020$bmi_usd_price
  
percent <- c(0,.01,.1,.5,.9,.99,1)
quantiles <- quantile(var,percent)
print(quantiles)
```

```{r}
get.var <- function(var_name, wmpb_World_BMI_2020) {
  # Assuming 'var_name' is a string representing the variable name to extract
  v <- wmpb_World_BMI[[var_name]] %>% 
    as.numeric()
  return(v)
}
```

```{r}
library(tmap)
tmap_mode("plot")
tmap_options(legend.width = 0.25) # Adjusts the width of the legend area

# Define the function
percentmap <- function(var_name, wmpb_World_BMI_2020, legtitle=NA, mtitle="Percentile Map") {
  percent <- c(0, .01, .1, .5, .9, .99, 1)
  var <- get.var(var_name, wmpb_World_BMI_2020)
  bperc <- quantile(var, percent, na.rm = TRUE)
  
  tm_shape(wmpb_World_BMI_2020) +
    tm_fill(var_name,
            title = legtitle,
            breaks = bperc,
            palette = "Greens",
            legend.hist = TRUE) +
    tm_borders() +
    tm_layout(main.title = mtitle, 
              main.title.position = "center",
              legend.position = c("left", "bottom")) -> tm
  
  print(tm)
}

# Execute the function
percentmap("bmi_usd_price", wmpb_World_BMI_2020, legtitle = "Percentile", mtitle = "BMI USD Percentile Map")

```

```{r}
boxbreaks <- function(v,mult=1.5) {
  qv <- unname(quantile(v))
  iqr <- qv[3] - qv[1]
  upfence <- qv[3] + mult * iqr
  lofence <- qv[1] - mult * iqr
  # initialize break points vector
  bb <- vector(mode="numeric",length=6)
  # logic for lower and upper fences
  if (lofence < qv[1]) {  # no lower outliers
    bb[1] <- lofence
    bb[2] <- floor(qv[1])
  } else {
    bb[2] <- lofence
    bb[1] <- qv[1]
  }
  if (upfence > qv[4]) { # no upper outliers
    bb[6] <- upfence
    bb[5] <- ceiling(qv[4])
  } else {
    bb[5] <- upfence
    bb[6] <- qv[4]
  }
  bb <- sort(bb)
  bb[2:4] <- qv[1:3]
  return(bb)
}
```

```{r}
get.var <- function(vname,df) {
  v <- df[vname] %>% st_set_geometry(NULL)
  v <- unname(v[,1])
  return(v)
}
```

```{r}
get.var <- function(vname, df) {
  # This assumes 'vname' is the name of the variable to extract,
  # and 'df' is the data frame or sf object without geometry.
  v <- df[[vname]] %>% as.numeric()  # Extract as vector and ensure it's numeric
  return(v)
}
library(sf)  # Assuming you're working with sf objects
library(dplyr)  # For the pipe operator

var <- na.omit(get.var("bmi_usd_price", wmpb_World_BMI))
bb <- boxbreaks(var)
print(bb)
```

```{r}
boxmap <- function(vnam, df, 
                   legtitle=NA,
                   mtitle="Box Map",
                   mult=1.5){
  var <- get.var(vnam,df)
  bb <- boxbreaks(var)
  tm_shape(df) +
    tm_polygons() +
  tm_shape(df) +
     tm_fill(vnam,title=legtitle,
             breaks=bb,
             palette="Oranges",
          labels = c("lower outlier", 
                     "< 25%", 
                     "25% - 50%", 
                     "50% - 75%",
                     "> 75%", 
                     "upper outlier"))  +
  tm_borders() +
  tm_layout(main.title = mtitle, 
            title.position = c("left",
                               "top"))
}
```

```{r}
tmap_mode("view")
boxmap("bmi_usd_price", wmpb_World_BMI_2020)+
  tm_facets(by="continent", 
            free.coords=TRUE, 
            drop.shapes=FALSE) 
```

```{r}
# First, ensure that "Seven seas (open ocean)" rows are removed.
wmpb_World_BMI_2020 <- wmpb_World_BMI_2020[wmpb_World_BMI_2020$continent != "Seven seas (open ocean)", ]

library(dplyr)

# Extract non-spatial data
non_spatial_data <- as.data.frame(wmpb_World_BMI_2020)

# Perform your filtering and factor level dropping
cleaned_data <- non_spatial_data %>%
  filter(continent != "Seven seas (open ocean)") %>%
  droplevels()

library(treemap)

treemap(cleaned_data,
        index = c("continent", "name"),
        vSize = "gdp_per_employed",
        vColor = "bmi_usd_price",
        type = "manual",
        palette="Blues",
        title = "BMI (USD) by Continent and GDP_per_employed, 2020",
        title.legend = "Big Mac Index ($USD)"
)

```

bivariate analysis

```{r}
#install.packages("ggplot2")
#install.packages("ggExtra")
library(ggplot2)
library(ggExtra)
```

```{r}
ggplot(world_bmi, aes(x = inflation, y = bmi_usd_price)) +
  geom_bin2d(bins = 50) +  # Adjust 'bins' to change the resolution of the heatmap
  scale_fill_viridis_c() +  # Use a perceptually uniform color scale
  labs(title = "Bivariate Heatmap of BMI USD Price and Inflation",
       x = "Inflation ($)",
       y = "BMI ($USD)",
       fill = "Count") +
  theme_minimal()

```

# 3 Time-Series using ARIMA (AutoRegressive Integrated Moving Average)

The ARIMA model is specified by three parameters: (p, d, q), where:

p = is the order of the autoregressive part, d = is the degree of first differencing involved, q = is the order of the moving average part.

In the context of Big Mac Index and other econometric variables, we leverage on ARIMA models to forecast future values of the Big Mac Index based on its historical data from 2002 - 2021. We will project the value of `bmi_usd_price` to 2030.

## 3.1 Calling out Forecast Packages

```{r}
library(forecast)
library(tibble)
library(dplyr)
```

## 3.2 Filter Data to Time Series Object

```{r}
uk_data <- big_mac %>%
  filter(country == "United Kingdom", year >= 2002, year <= 2022) %>%
  arrange(year) %>%
  select(year, bmi_usd_price)

# Convert to ts object
uk_ts <- ts(uk_data$bmi_usd_price, start = 2002, end = 2021, frequency = 2)
```

## 3.3 Model Fitting using `auto.arima`

Use the auto.arima function to automatically select the best ARIMA model based on AIC or BIC.

-   Lower AIC or BIC values indicate a better-fitting model.
-   AIC focuses on the goodness of fit but can suffer from overfitting.
-   BIC adds a penalty term for the number of parameters in the model, which can result in choosing simpler models than AIC.

```{r}
library(forecast)

# The auto.arima function will search through different ARIMA models 
fit <- auto.arima(uk_ts)

# the summary of the fitted model which includes the AIC and BIC values
summary(fit)

# compare AIC and BIC manually if needed
aic_value <- fit$aic
bic_value <- fit$bic

# Print the AIC and BIC values
cat("AIC:", aic_value, "\n")
cat("BIC:", bic_value, "\n")

fit_bic <- auto.arima(uk_ts, ic = "bic")

# Print summary of the model selected based on BIC
summary(fit_bic)

```

ACF and PACF plots identify whether an *AR, MA, or ARMA* model will be more appropriate:

**AR (Autoregressive) model**: The Partial Autocorrelation Function (PACF) plot would show a significant spike at the lag corresponding to the order of the AR term (p), and then it would cut off, meaning other spikes should not be significant. The Autocorrelation Function (ACF) plot would show a more gradual decline.

**MA (Moving Average) model**: The Autocorrelation Function (ACF) plot would show a significant spike at the lag corresponding to the order of the MA term (q), and then it would cut off. The Partial Autocorrelation Function (PACF) would show a more gradual decline.

**ARMA (Autoregressive Moving Average) model**: Both ACF and PACF plots show a more complex pattern, with neither cutting off abruptly, indicating a mixture of AR and MA behaviours.

To determine the specific AR or MA orders, it would typically look for the number of lags before the plot crosses the significance boundary (the blue dashed lines). Spikes that go beyond this boundary are considered significant.

When you're looking at your ACF and PACF plots, consider the following:

If there are a few significant spikes in the PACF followed by non-significant ones, and the ACF tails off, consider an AR model with the order determined by the number of significant spikes. If there are a few significant spikes in the ACF followed by non-significant ones, and the PACF tails off, consider an MA model with the order determined by the number of significant spikes. If both the ACF and PACF show a complex pattern without a clear cutoff, an ARMA model might be needed, where it would need to experiment with different orders to find the best fit.

```{r}
# Load the necessary library
library(forecast)

# Assuming uk_ts is your time series object
# Plot the ACF
acf(uk_ts)

# Plot the PACF
pacf(uk_ts)
```

## 3.4 Forecasting and plotting

Forecasting the `bmi_usd_price` up to 2030

```{r}
# Load the necessary libraries
library(forecast)

# Read in your time series data
uk_ts <- ts(uk_data$bmi_usd_price, start = 2002, end = 2021, frequency = 1)

# Fit an ARIMA model using auto.arima
auto_fit <- auto.arima(uk_ts)

# Forecast from 2021 to 2030
forecast_length <- 2030 - max(time(uk_ts))
forecast_result <- forecast(auto_fit, h=forecast_length)

# Plot the forecast with actual data
plot(forecast_result)
lines(uk_ts, type="o", col='blue')
```

In the context of ARIMA models, the **`order`** argument specifies the order of the model as a triplet of parameters (p, d, q):

1.  **p**: The number of autoregressive (AR) terms. It represents the number of lags of the dependent variable that are used as predictors. For instance, if p is 3, the predictors for x(t) will be x(t−1),x(t−2), and x(t−3).

2.  **d**: The degree of differencing. It indicates the number of times the data have had past values subtracted. Differencing is used to make the series stationary, which is a requirement for many statistical modelling methods.

3.  **q**: The number of moving average (MA) terms. This parameter is associated with the number of lagged forecast errors in the prediction equation. For example, if q is 2, the prediction of x(t) will be adjusted by the forecast errors made on the two previous predictions x(t−1) and x(t−2).

So when **`order=c(1,1,1) is in use`**, it specifies an ARIMA model with:

-   1 AR term (p=1)

-   1 degree of differencing (d=1)

-   1 MA term (q=1)

The choice of these parameters is critical as they define the structure of the time series model you are trying to fit. The ARIMA model attempts to describe the autocorrelations in the data with these parameters.

```{r}
# If you want to manually specify the model, fit it like so:
manual_fit <- Arima(uk_ts, order=c(2,1,8))



# And forecast manually specified model from 2021 to 2030
manual_forecast <- forecast(manual_fit, h=forecast_length, level=c(90, 95))

# Plot the manual forecast
plot(manual_forecast)
lines(uk_ts, type="o", col='red') 
```

Adding a slope/gradient to the forecast and the Confidence Interval L: 90% + U:95%

```{r}
# Fit a linear model to your time series data
trend_model <- lm(uk_ts ~ time(uk_ts))

# If you want to manually specify the model, fit it like so:
manual_fit <- Arima(uk_ts, order=c(2,1,8))

# And forecast manually specified model from 2021 to 2030
manual_forecast <- forecast(manual_fit, h=forecast_length, level=c(90, 95))

lower_90 <- manual_forecast$lower[,1]  # 90% confidence interval lower bound
upper_90 <- manual_forecast$upper[,1]  # 90% confidence interval upper bound
lower_95 <- manual_forecast$lower[,2]  # 95% confidence interval lower bound
upper_95 <- manual_forecast$upper[,2]  # 95% confidence interval upper bound

# To display these values, you could use a data frame
confidence_intervals <- data.frame(
  Time = time(manual_forecast$mean),
  Lower_90 = lower_90,
  Upper_90 = upper_90,
  Lower_95 = lower_95,
  Upper_95 = upper_95
)

print(confidence_intervals)
# Plot the manual forecast
plot(manual_forecast, main = "UK's Forecasted BMI value with CI",  # Add a title
     xlab = "Year",  # Label for the x-axis
     ylab = "BMI_USDprice",  # Label for the y-axis
     col = "blue",  # Color for the forecast line
     type = "n",
     lwd = 1,  # Width of the forecast line
     cex.lab = 0.8,  # Size of axis labels
     cex.axis = 0.8,  # Size of axis tick labels
     cex.main = 1) 
lines(manual_forecast$mean, col = "blue", lwd = 2)
points(uk_ts, pch = 19, col = "red")
lines(uk_ts, type="o", col='red')

# Add a linear trend line to the plot
abline(trend_model$coefficients, col="grey")
legend("topleft", legend = c("Forecast", "Historical Data", "Trend Line"), col = c("blue", "red", "darkgrey"), lty = 1, lwd = 0.5, pch = c(NA, 19, NA))

# Get the slope (gradient) of the linear model
slope <- coef(trend_model)["time(uk_ts)"]

# Add the slope value as text on the plot
# You can adjust the x and y coordinates and the text size (cex) as needed
text(x = mean(time(uk_ts)), y = max(uk_ts), labels = paste("Gradient:", round(slope, 3)), pos = 3, cex = 0.8)

x_offset <- 16

ci_x <- c(time(manual_forecast$mean), rev(time(manual_forecast$mean)))
ci_y <- c(manual_forecast$lower[,2], rev(manual_forecast$upper[,2]))
polygon(ci_x, ci_y, col = rgb(0, 1, 1, alpha = 0.1), border = NA)
```

```{r}
library(plotly)
# Assuming manual_forecast, trend_model, and uk_ts are already defined

# Convert uk_ts to a data frame for plotly (if it's not already)
uk_ts_df <- data.frame(Time = as.numeric(time(uk_ts)), Value = as.numeric(uk_ts))

# Create base plot with plotly
p <- plot_ly() %>%
  add_lines(data = uk_ts_df, x = ~Time, y = ~Value, name = "Historical Data", line = list(color = 'red')) %>%
  add_lines(x = time(manual_forecast$mean), y = manual_forecast$mean, name = "Forecast", line = list(color = 'blue')) %>%
  add_ribbons(x = time(manual_forecast$mean), ymin = manual_forecast$lower[,2], ymax = manual_forecast$upper[,2], name="95% CI", line = list(color = 'transparent'), fillcolor = 'rgba(0, 0, 255, 0.1)') %>%
  layout(title = "UK's Forecasted BMI value with CI", xaxis = list(title = "Year"), yaxis = list(title = "BMI_USDprice"))

# Add trend line - calculate points based on the linear model coefficients
trend_line_df <- data.frame(Time = c(min(uk_ts_df$Time), max(uk_ts_df$Time)))
trend_line_df$Trend <- coef(trend_model)["(Intercept)"] + coef(trend_model)["time(uk_ts)"] * trend_line_df$Time

# Add trend line to the plot
p <- p %>% add_lines(data = trend_line_df, x = ~Time, y = ~Trend, name = "Trend Line", line = list(color = 'grey'))

# Optionally, add annotations for the slope and CI labels
# Note: Adjust the coordinates (x, y) based on your data's range
p <- p %>%
  layout(annotations = list(
    list(x = mean(uk_ts_df$Time), y = max(uk_ts_df$Value), text = paste("Gradient:", round(slope, 3)), showarrow = F),
    list(x = max(uk_ts_df$Time), y = min(manual_forecast$lower[length(manual_forecast$mean),2]), text = paste("Lower 95%: Mean", round(manual_forecast$lower[length(manual_forecast$mean),2], 2)), showarrow = F),
    list(x = max(uk_ts_df$Time), y = max(manual_forecast$upper[length(manual_forecast$mean),2]), text = paste("Upper 95%: Mean", round(manual_forecast$upper[length(manual_forecast$mean),2], 2)), showarrow = F)
  ))

# Render the plot
p

```

## 3.5 Residual Testing

### 3.5.1 Checking Normality through `Shapiro.Test`

```{r}
# Assuming 'fit' is your fitted ARIMA model
residuals <- residuals(manual_fit)

# Plot histogram
hist(residuals, breaks = 30, main = "Histogram and Density Plot of Residuals", xlab = "Residuals", col = "skyblue", border = "white", probability = TRUE)

# Overlay density plot
lines(density(residuals), col = "red", lwd = 2)

```

```{r}
# Shapiro-Wilk normality test
shapiro.test(residuals)
```

W: This is the test statistic value, where W ranges from 0 to 1. A value of 1 indicates perfect normality.

p-value = 0.8108: A common threshold for significance is 0.05. If the p-value is less than 0.05, we reject the null hypothesis and conclude that the data do not come from a normally distributed population. If the p-value is greater than 0.05.

In this result, with a p-value of 0.8108, there is strong evidence to suggest that residuals are normally distributed.

### 3.5.2 Using QQ-plot

A Q-Q (quantile-quantile) plot compares the distribution of residuals to a normal distribution.

```{r}
library(car)

# Create Q-Q plot with statistical annotations
qqPlot(residuals(manual_fit), main="Q-Q Plot of Residuals", 
       ylab="Sample Quantiles", 
       las=1)


```

### 3.5.3 Checking for autocorrelation at any given lags

The Ljung-Box test provides a p-value that can be used to determine whether there is significant evidence of autocorrelation at any of the lags tested.

```{r}
# Perform the Ljung-Box test and store the result
lb_test <- Box.test(residuals(fit), type="Ljung-Box")

# Print the test statistic and p-value
cat("Ljung-Box test statistic:", lb_test$statistic, "\n")
cat("P-value:", lb_test$p.value, "\n")

```

In the context of the Ljung-Box test, a higher p-value is generally considered better when using it to check the residuals of a time series model for randomness or lack of autocorrelation. The Ljung-Box test is a type of statistical test that is used to determine if there are significant autocorrelations at lag in the residuals of a time series model.

-   *Higher p-value* (typically \> 0.05): Suggests that the residuals are random, indicating that the model has adequately captured the information in the data. In other words, there is no evidence of significant autocorrelation at any of the tested lags, and the model fits well.

-   *Lower p-value* (typically ≤ 0.05): Suggests that there is significant autocorrelation in the residuals that the model has not captured. This can indicate that the model could be improved, either by adding more terms or by considering a different model structure.

Therefore, we can observe that the p-value is above 0.138 and hence we can reject the null hypothesis that there is a autocorrelation. This outcome would suggest that our model is capturing the underlying patterns in the data well, and the residuals are essentially white noise, as desired.

```{r}
# Plotting ACF of residuals
Acf(residuals(manual_fit), main='ACF of Residuals')

# Conducting the Ljung-Box test
Box.test(residuals(manual_fit), lag = log(length(residuals(fit))))

```

For ACF plot of residuals, the goal is to see no pattern and have all bars within the confidence interval, indicating that the residuals are random (white noise) and the model has done a good job of capturing the underlying patterns in the data.

If there is significant autocorrelations, there is a need to revise the model by adjusting its parameters or by considering additional explanatory variables.

::: callout-info
For the purpose of Shiny App, all Countries will run ARIMA with the option for the viewer to select the (1) model fit criteria, (2) parameters and (3) residual checks to see if the model suffers from overfitting or autocorellation.
:::
