[
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex01/Take-Home_Exercise01.html",
    "href": "Take-home_Exercises/Take-Home_Ex01/Take-Home_Exercise01.html",
    "title": "Creating Data Visualisation Beyond Default",
    "section": "",
    "text": "OECD education director Andreas Schleicher shared in a BBC article that “Singapore managed to achieve excellence without wide differences between children from wealthy and disadvantaged families.” (2016) Furthermore, several Singapore’s Minister for Education also started an “every school a good school” slogan. The general public, however, strongly believes that there are still disparities that exist, especially between the elite schools and neighborhood school, between students from families with higher socioeconomic status and those with relatively lower socioeconomic status and immigration and non-immigration families."
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex01/Take-Home_Exercise01.html#data-sets",
    "href": "Take-home_Exercises/Take-Home_Ex01/Take-Home_Exercise01.html#data-sets",
    "title": "Creating Data Visualisation Beyond Default",
    "section": "3.1 Data Sets",
    "text": "3.1 Data Sets\nThe PISA 2022 database contains the full set of responses from individual students, school principals and parents. A total of five data sets were extracted and their contents are as follows:\n\nStudent questionnaire data file\nSchool questionnaire data file\nTeacher questionnaire data file\nCognitive item data file\nQuestionnaire timing data file\n\nIn this exercise, we are to utilize the student questionnaire data set only."
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex01/Take-Home_Exercise01.html#data-preparation",
    "href": "Take-home_Exercises/Take-Home_Ex01/Take-Home_Exercise01.html#data-preparation",
    "title": "Creating Data Visualisation Beyond Default",
    "section": "3.2 Data Preparation",
    "text": "3.2 Data Preparation\n\n3.2.1 Installing R Packages\nInstallation of mulitple R packages via the use of pacman::p_load() function from the pacman package. See the code chunk below:\n\n\nCode\npacman::p_load(tidyverse, haven, dplyr, plyr, ggrepel, ggthemes, knitr, kableExtra, intsvy, hrbrthemes, ggridges, ggdist, patchwork, colorspace, reshape2, scales, ggplot2, ggpol, gridExtra)\n\n\n\n\n3.2.2 Uploading & Filtering the Data Set\nThe students questionaire data file was uploaded as stu_qqq_SG. See the code chunk below:\n\n\nCode\nstu_qqq &lt;- read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\nstu_qqq_SG &lt;- stu_qqq %&gt;% filter(CNT ==\"SGP\")\nwrite_rds(stu_qqq_SG,\"data/stu_qqq_SG.rds\")\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")\nhead(stu_qqq_SG, 5)\n\n\n# A tibble: 5 × 1,279\n  CNT   CNTRYID CNTSCHID CNTSTUID CYC   NatCen STRATUM SUBNATIO REGION  OECD\n  &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 SGP       702 70200052 70200001 08MS  070200 SGP01   7020000   70200     0\n2 SGP       702 70200134 70200002 08MS  070200 SGP01   7020000   70200     0\n3 SGP       702 70200112 70200003 08MS  070200 SGP01   7020000   70200     0\n4 SGP       702 70200004 70200004 08MS  070200 SGP01   7020000   70200     0\n5 SGP       702 70200152 70200005 08MS  070200 SGP01   7020000   70200     0\n# ℹ 1,269 more variables: ADMINMODE &lt;dbl&gt;, LANGTEST_QQQ &lt;dbl&gt;,\n#   LANGTEST_COG &lt;dbl&gt;, LANGTEST_PAQ &lt;dbl&gt;, Option_CT &lt;dbl&gt;, Option_FL &lt;dbl&gt;,\n#   Option_ICTQ &lt;dbl&gt;, Option_WBQ &lt;dbl&gt;, Option_PQ &lt;dbl&gt;, Option_TQ &lt;dbl&gt;,\n#   Option_UH &lt;dbl&gt;, BOOKID &lt;dbl&gt;, ST001D01T &lt;dbl&gt;, ST003D02T &lt;dbl&gt;,\n#   ST003D03T &lt;dbl&gt;, ST004D01T &lt;dbl&gt;, ST250Q01JA &lt;dbl&gt;, ST250Q02JA &lt;dbl&gt;,\n#   ST250Q03JA &lt;dbl&gt;, ST250Q04JA &lt;dbl&gt;, ST250Q05JA &lt;dbl&gt;, ST250D06JA &lt;chr&gt;,\n#   ST250D07JA &lt;chr&gt;, ST251Q01JA &lt;dbl&gt;, ST251Q02JA &lt;dbl&gt;, ST251Q03JA &lt;dbl&gt;, …\n\n\nFocusing on the Exercise Objectives, the Data of Interests will be scoped towards selected variables. These variables are:\n\nSTRATUM\nGENDER\nIMMIGRANT STATUS\nSES STATUS\nPV Values\n\n\n\nCode\nstu_qqq_SG_selectedV &lt;- stu_qqq_SG %&gt;% select(CNTSTUID, STRATUM, ST004D01T, IMMIG, ESCS,PV1READ:PV10READ, PV1SCIE:PV10SCIE, PV1MATH:PV10MATH)\n\n\nThe variables are all anchored based on each Student’s UNIQUE ID - CNTSTUID\n\n\n3.2.3 Checking Data Structure and Coherency\nNext, we will check for duplicates, missing values and convert data types as part of data pre-processing.\nData Health Handling duplicates\n\n\nCode\nstu_qqq_SG_selectedV[duplicated(stu_qqq_SG_selectedV),]\n\n\n# A tibble: 0 × 35\n# ℹ 35 variables: CNTSTUID &lt;dbl&gt;, STRATUM &lt;chr&gt;, ST004D01T &lt;dbl&gt;, IMMIG &lt;dbl&gt;,\n#   ESCS &lt;dbl&gt;, PV1READ &lt;dbl&gt;, PV2READ &lt;dbl&gt;, PV3READ &lt;dbl&gt;, PV4READ &lt;dbl&gt;,\n#   PV5READ &lt;dbl&gt;, PV6READ &lt;dbl&gt;, PV7READ &lt;dbl&gt;, PV8READ &lt;dbl&gt;, PV9READ &lt;dbl&gt;,\n#   PV10READ &lt;dbl&gt;, PV1SCIE &lt;dbl&gt;, PV2SCIE &lt;dbl&gt;, PV3SCIE &lt;dbl&gt;, PV4SCIE &lt;dbl&gt;,\n#   PV5SCIE &lt;dbl&gt;, PV6SCIE &lt;dbl&gt;, PV7SCIE &lt;dbl&gt;, PV8SCIE &lt;dbl&gt;, PV9SCIE &lt;dbl&gt;,\n#   PV10SCIE &lt;dbl&gt;, PV1MATH &lt;dbl&gt;, PV2MATH &lt;dbl&gt;, PV3MATH &lt;dbl&gt;, PV4MATH &lt;dbl&gt;,\n#   PV5MATH &lt;dbl&gt;, PV6MATH &lt;dbl&gt;, PV7MATH &lt;dbl&gt;, PV8MATH &lt;dbl&gt;, …\n\n\nChecking missing values\n\n\nCode\nsum(is.na(stu_qqq_SG_selectedV))\n\n\n[1] 283\n\n\nThere are a total of 283 missing values which is merely 4.2% of the overall data set hence can regarded as statistically negligible.\nConverting Data Types: Converting CNTSTUID & GENDER from num type to chrtype as they are categorical in nature. Subsequently, renaming and recoding them to enhance data comprehension.\n\n\nCode\nstu_qqq_SG_selectedV&lt;- stu_qqq_SG_selectedV %&gt;% mutate(CNTSTUID = as.character(CNTSTUID))\n\nnames(stu_qqq_SG_selectedV)[names(stu_qqq_SG_selectedV) == 'CNTSTUID'] &lt;- 'STUDENT ID'\n\nnames(stu_qqq_SG_selectedV)[names(stu_qqq_SG_selectedV) == 'ST004D01T'] &lt;- 'GENDER'\n\nstu_qqq_SG_selectedV &lt;- stu_qqq_SG_selectedV %&gt;%\n  mutate(GENDER = recode(as.character(GENDER), '1' = 'FEMALE', '2' = 'MALE'))\n\nstu_qqq_SG_selectedV &lt;- stu_qqq_SG_selectedV %&gt;%\n  mutate(STRATUM = recode(STRATUM, 'SGP01' = 'MAINSTREAM SCH', 'SGP03' = 'PRIVATE SCH'))\n\nstu_qqq_SG_selectedV&lt;- stu_qqq_SG_selectedV %&gt;%\n  mutate(IMMIG = recode(IMMIG, '1' = 'NATIVE', '2' = '2ND GEN', '3' = '1ST GEN'))"
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex01/Take-Home_Exercise01.html#explore-score-distribution-across-all-subjects",
    "href": "Take-home_Exercises/Take-Home_Ex01/Take-Home_Exercise01.html#explore-score-distribution-across-all-subjects",
    "title": "Creating Data Visualisation Beyond Default",
    "section": "4.1 Explore Score Distribution across all subjects",
    "text": "4.1 Explore Score Distribution across all subjects\n\n4.1.1 Observations through Prob Density and Histogram\nUtilising the prob density and histogram, we develop the visualisation to observe the distributions and the summary stats across all subject of interests.\n\nAll Subjects\n\n\nCode\nstu_qqq_SG_selectedV &lt;- stu_qqq_SG_selectedV %&gt;%\n  mutate(Maths = rowSums(stu_qqq_SG_selectedV[paste0('PV', c(1:10), \"MATH\")], \n                   na.rm = TRUE)/100) %&gt;% \n  mutate(Reading = \n           rowSums(stu_qqq_SG[paste0('PV', c(1:10), \"READ\")], \n                   na.rm = TRUE)/100) %&gt;% \n  mutate(Science = \n           rowSums(stu_qqq_SG[paste0('PV', c(1:10), \"SCIE\")], \n                   na.rm = TRUE)/100)\ntemp_Data &lt;- stu_qqq_SG_selectedV[, c(\"Science\", \"Reading\", \"Maths\")]\ntemp_Data &lt;- melt(temp_Data, variable.name = \"Subject\")\n\n\nNo id variables; using all as measure variables\n\n\nCode\nggplot(temp_Data, aes(x = value, y = Subject)) +\n  stat_halfeye(aes(fill = Subject), \n               adjust = 0.5,\n               justification = 0.1,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", shape = 16, \n               size = 3, color = \"darkred\", \n               position = position_nudge(x = 0.0)) +\n  stat_summary(fun = mean, colour=\"darkred\", \n               geom = \"text\", show.legend = FALSE, \n               vjust = -1.5, aes( label=round(after_stat(x), 1))) +\n  labs(y = NULL, x = \"Scores\",\n       title = \"Distribution of Scores\",\n       subtitle = \"Math subject, yielded greater performance by students compared to Reading and Science\") +\n  theme_tidybayes()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nMaths Score\n\n\nCode\n# Calculate mean and median\nmean_value &lt;- mean(stu_qqq_SG_selectedV$Maths, na.rm = TRUE)\nmedian_value &lt;- median(stu_qqq_SG_selectedV$Maths, na.rm = TRUE)\n\n# Create the histogram\nhistogram_plot &lt;- ggplot(data = stu_qqq_SG_selectedV, aes(x = Maths)) +\n  geom_histogram(binwidth = 2, fill = \"lightblue\", color = \"black\") +  # Adjust binwidth and color here\n  geom_vline(xintercept = mean_value, color = \"red\", linetype = \"dashed\", linewidth = 0.5) +\n  geom_vline(xintercept = median_value, color = \"blue\", linetype = \"dotted\", linewidth = 0.5) +\n  labs(title = \"Histogram of Maths Scores\",\n       x = \"Maths Scores\",\n       y = \"Frequency\",\n       subtitle = paste(\"Mean (red):\", round(mean_value, 2), \n                        \"- Median (blue):\", round(median_value, 2)))\n\n# Print the plot\nprint(histogram_plot)  \n\n\n\n\n\n\n\nReading\n\n\nCode\n# Calculate mean and median\nmean_value &lt;- mean(stu_qqq_SG_selectedV$Reading, na.rm = TRUE)\nmedian_value &lt;- median(stu_qqq_SG_selectedV$Reading, na.rm = TRUE)\n\n# Create the histogram\nhistogram_plot &lt;- ggplot(data = stu_qqq_SG_selectedV, aes(x = Reading)) +\n  geom_histogram(binwidth = 2, fill = \"lightgreen\", color = \"black\") +  # Adjust binwidth and color here\n  geom_vline(xintercept = mean_value, color = \"red\", linetype = \"dashed\", linewidth = 0.5) +\n  geom_vline(xintercept = median_value, color = \"blue\", linetype = \"dotted\", linewidth = 0.5) +\n  labs(title = \"Histogram of Reading Scores\",\n       x = \"Reading Scores\",\n       y = \"Frequency\",\n       subtitle = paste(\"Mean (red):\", round(mean_value, 2), \n                        \"- Median (blue):\", round(median_value, 2)))\n\n# Print the plot\nprint(histogram_plot)  \n\n\n\n\n\n\n\nScience\n\n\nCode\n# Calculate mean and median\nmean_value &lt;- mean(stu_qqq_SG_selectedV$Science, na.rm = TRUE)\nmedian_value &lt;- median(stu_qqq_SG_selectedV$Science, na.rm = TRUE)\n\n# Create the histogram\nhistogram_plot &lt;- ggplot(data = stu_qqq_SG_selectedV, aes(x = Science)) +\n  geom_histogram(binwidth = 2, fill = \"lightpink\", color = \"black\") +  # Adjust binwidth and color here\n  geom_vline(xintercept = mean_value, color = \"red\", linetype = \"dashed\", linewidth = 0.5) +\n  geom_vline(xintercept = median_value, color = \"blue\", linetype = \"dotted\", linewidth = 0.5) +\n  labs(title = \"Histogram of Science Scores\",\n       x = \"Science Scores\",\n       y = \"Frequency\",\n       subtitle = paste(\"Mean (red):\", round(mean_value, 2), \n                        \"- Median (blue):\", round(median_value, 2)))\n\n# Print the plot\nprint(histogram_plot)"
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex01/Take-Home_Exercise01.html#insights-observations",
    "href": "Take-home_Exercises/Take-Home_Ex01/Take-Home_Exercise01.html#insights-observations",
    "title": "Creating Data Visualisation Beyond Default",
    "section": "4.1.2 Insights & Observations",
    "text": "4.1.2 Insights & Observations\n\nThe aim of these visualisations was to show the performance distribution of students across all individual subjects.\nTrends in each plot showed :\n\ndistribution characteristic is of a Normal Distribution,\ndistribution mildly skews to the left.\nFrom here we can deduce that just about more than half of the cohort scores above the 50% mark.\n\nStudents fair better in Math, as a subject, compared to Reading and Science. :::"
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex01/Take-Home_Exercise01.html#exploring-performance-score-by-gender",
    "href": "Take-home_Exercises/Take-Home_Ex01/Take-Home_Exercise01.html#exploring-performance-score-by-gender",
    "title": "Creating Data Visualisation Beyond Default",
    "section": "4.2 Exploring Performance Score by Gender",
    "text": "4.2 Exploring Performance Score by Gender\n\n4.2.1 Observations through boxplots\nUtilising the boxplots, we visualise the how gender fair across all subjects.\n\nMathReadingScience\n\n\n\n\nCode\nlibrary(ggplot2)\n\n# Calculate mean and median for each gender\nmeans &lt;- aggregate(Maths ~ GENDER, data = stu_qqq_SG_selectedV, FUN = mean)\n\n# Create the boxplot\nggplot(data = stu_qqq_SG_selectedV, aes(y = Maths, x = GENDER, color = GENDER)) +\n  geom_boxplot(width = 0.3) +\n  \n  # Add mean lines\n  geom_errorbar(data = means, aes(ymin = Maths, ymax = Maths, x = GENDER), \n                width = 0.2, color = \"darkred\") +\n  geom_text(data = means, aes(label = round(Maths, 1), y = Maths, x = GENDER), \n            vjust = -1.5, color = \"darkred\", size = 3) +\n  \n  # Titles and subtitles\n  labs(title = \"Maths Scores by Gender\",\n       subtitle = \"Male scores better in Maths in comparison with Female\",\n       x = \"Gender\",\n       y = \"Maths Scores\",\n       color = \"Gender\")\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\n\n# Calculate mean and median for each gender\nmeans &lt;- aggregate(Reading ~ GENDER, data = stu_qqq_SG_selectedV, FUN = mean)\n\n# Create the boxplot\nggplot(data = stu_qqq_SG_selectedV, aes(y = Reading, x = GENDER, color = GENDER)) +\n  geom_boxplot(width = 0.3) +\n  \n  # Add mean lines\n  geom_errorbar(data = means, aes(ymin = Reading, ymax = Reading, x = GENDER), \n                width = 0.2, color = \"darkred\") +\n  geom_text(data = means, aes(label = round(Reading, 1), y = Reading, x = GENDER), \n            vjust = -1.5, color = \"darkred\", size = 3) +\n  \n  # Titles and subtitles\n  labs(title = \"Reading Scores by Gender\",\n       subtitle = \"Female scores better in Reading\",\n       x = \"Gender\",\n       y = \"Reading Scores\",\n       color = \"Gender\")\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\n\n# Calculate mean and median for each gender\nmeans &lt;- aggregate(Science ~ GENDER, data = stu_qqq_SG_selectedV, FUN = mean)\n\n# Create the boxplot\nggplot(data = stu_qqq_SG_selectedV, aes(y = Science, x = GENDER, color = GENDER)) +\n  geom_boxplot(width = 0.3) +\n  \n  # Add mean lines\n  geom_errorbar(data = means, aes(ymin = Science, ymax = Science, x = GENDER), \n                width = 0.2, color = \"darkred\") +\n  geom_text(data = means, aes(label = round(Science, 1), y = Science, x = GENDER), \n            vjust = -1.5, color = \"darkred\", size = 3) +\n  \n  # Titles and subtitles\n  labs(title = \"Science Scores by Gender\",\n       subtitle = \"Male scores better in Science as compared to their Female counterparts\",\n       x = \"Gender\",\n       y = \"Science Scores\",\n       color = \"Gender\")\n\n\n\n\n\n\n\n\n\n\n4.2.2 Insights & Observations\n\nThe aim of these visualisations was to show Gender performance across all individual subjects.\nMales Mean scores are higher hence we can deduce that Males fair better in Math and Science subjects.\nFemale fair better in Reading as compared to their Male counterparts."
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex01/Take-Home_Exercise01.html#exploring-performance-score-by-se-index",
    "href": "Take-home_Exercises/Take-Home_Ex01/Take-Home_Exercise01.html#exploring-performance-score-by-se-index",
    "title": "Creating Data Visualisation Beyond Default",
    "section": "4.3 Exploring Performance Score by SE Index",
    "text": "4.3 Exploring Performance Score by SE Index\n\n4.3.1 SE Distribution by Scores\nSE Distribution Let’s first take a look into the socioeconomic (SE) distribution to gain some insights.\n\n\nCode\nstu_qqq_SG_selectedV &lt;- na.omit(stu_qqq_SG_selectedV)\nggplot(data = stu_qqq_SG_selectedV, aes(x = ESCS)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0, point_colour = NA) +\n  geom_boxplot(width = 0.20,\n               outlier.shape = NA) +\n  labs(y = NULL,\n       title = \"SE Distribution\", subtitle = \"Left-Skewed SE Distribution\")  +\n  theme_tidybayes()\n\n\n\n\n\nESCS Index by Subject Performance\n\n\nCode\ncor1 &lt;- round(cor(stu_qqq_SG_selectedV$Maths, stu_qqq_SG_selectedV$ESCS),2)\ncor2 &lt;- round(cor(stu_qqq_SG_selectedV$Science, stu_qqq_SG_selectedV$ESCS),2)\ncor3 &lt;- round(cor(stu_qqq_SG_selectedV$Reading, stu_qqq_SG_selectedV$ESCS),2)\n\nse1 &lt;- ggplot(data = stu_qqq_SG_selectedV,\n             aes(y = Maths, x = ESCS)) +\n        geom_point(size = 0.1)+\n        geom_smooth(method = lm) + \n        annotate(\"text\", x = 2.5, y = 100, label=paste0(\"r = \", cor1), color = 'lightblue') +\n        theme_tidybayes()\n\nse2 &lt;- ggplot(data = stu_qqq_SG_selectedV,\n             aes(y = Science, x = ESCS)) +\n        geom_point(size = 0.1)+\n        geom_smooth(method = lm) +\n        annotate(\"text\", x = 2.5, y = 100, label=paste0(\"r = \", cor2), color = 'lightgreen') + \n        theme_tidybayes()\n\nse3 &lt;- ggplot(data = stu_qqq_SG_selectedV,\n             aes(y = Reading, x = ESCS)) +\n        geom_point(size = 0.1)+\n        geom_smooth(method = lm) + \n        annotate(\"text\", x = 2.5, y = 100, label=paste0(\"r = \", cor3), color = 'lightpink') +\n        theme_tidybayes()\n\nse1/se2/se3\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n4.3.2 Insights & Observations\n\nLeft Skewed Distribution from the SE Index can be deduced that students with normal to high ESCS Index are more prevalent as compared to the lower ESCS index. To bring the conversation further, we can study into the distribution of native, 1st Gen and 2nd Gen by the ESCS index in the future.\nDepicts a positive moderate correlation between Scores and ESCS index, hence we can infer that students with higher ESCS Index scores better in their subjects."
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex02/Take-Home_Exercise02.html",
    "href": "Take-home_Exercises/Take-Home_Ex02/Take-Home_Exercise02.html",
    "title": "Viz- Makeover Exercise",
    "section": "",
    "text": "This document serves as a submission for Take-home Exercise 2 as required by the course ISSS608 Visual Analytics and Applications.\nIn this exercise, we aim to apply the different data visualization design practices and principles and improve on the Take-home Exercise 1 output of a fellow classmate. Context for Exercise 1 as shown below.\nTask Context OECD education director Andreas Schleicher shared in a BBC article that “Singapore managed to achieve excellence without wide differences between children from wealthy and disadvantaged families.” (2016) Furthermore, several Singapore’s Minister for Education also started an “every school a good school” slogan. The general public, however, strongly believes that there are still disparities that exist, especially between the elite schools and neighborhood school, between students from families with higher socioeconomic status and those with relatively lower socioeconomic status and immigration and non-immigration families**.\nTask 1 Objectives The exercise task take-home exercise 1 are: to use appropriate Exploratory Data Analysis (EDA) methods and ggplot2 functions to reveal (1) Singapore students’ performance in mathematics, reading, and science and (2) relationship between these performances with schools, gender and socioeconomic status (SES) of the students."
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex02/Take-Home_Exercise02.html#installation-of-packages-following-the-code-chunk-below",
    "href": "Take-home_Exercises/Take-Home_Ex02/Take-Home_Exercise02.html#installation-of-packages-following-the-code-chunk-below",
    "title": "Viz- Makeover Exercise",
    "section": "2.1 Installation of packages following the code chunk below:",
    "text": "2.1 Installation of packages following the code chunk below:\n\n\nCode\npacman::p_load(tidyverse, haven, dplyr, plyr, ggrepel, ggthemes, knitr, kableExtra, intsvy, hrbrthemes, ggridges, ggdist, patchwork, colorspace, reshape2, scales, ggplot2, ggpol, gridExtra)"
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex02/Take-Home_Exercise02.html#importing-the-data",
    "href": "Take-home_Exercises/Take-Home_Ex02/Take-Home_Exercise02.html#importing-the-data",
    "title": "Viz- Makeover Exercise",
    "section": "2.2 Importing the data",
    "text": "2.2 Importing the data\nImporting data set and filtering data from Singapore Students only.\n\n\nCode\nstu_qqq &lt;- read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\nstu_qqq_SG &lt;- stu_qqq %&gt;% filter(CNT ==\"SGP\")\nwrite_rds(stu_qqq_SG,\"data/stu_qqq_SG.rds\")\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")\nhead(stu_qqq_SG, 5)\n\n\n# A tibble: 5 × 1,279\n  CNT   CNTRYID CNTSCHID CNTSTUID CYC   NatCen STRATUM SUBNATIO REGION  OECD\n  &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 SGP       702 70200052 70200001 08MS  070200 SGP01   7020000   70200     0\n2 SGP       702 70200134 70200002 08MS  070200 SGP01   7020000   70200     0\n3 SGP       702 70200112 70200003 08MS  070200 SGP01   7020000   70200     0\n4 SGP       702 70200004 70200004 08MS  070200 SGP01   7020000   70200     0\n5 SGP       702 70200152 70200005 08MS  070200 SGP01   7020000   70200     0\n# ℹ 1,269 more variables: ADMINMODE &lt;dbl&gt;, LANGTEST_QQQ &lt;dbl&gt;,\n#   LANGTEST_COG &lt;dbl&gt;, LANGTEST_PAQ &lt;dbl&gt;, Option_CT &lt;dbl&gt;, Option_FL &lt;dbl&gt;,\n#   Option_ICTQ &lt;dbl&gt;, Option_WBQ &lt;dbl&gt;, Option_PQ &lt;dbl&gt;, Option_TQ &lt;dbl&gt;,\n#   Option_UH &lt;dbl&gt;, BOOKID &lt;dbl&gt;, ST001D01T &lt;dbl&gt;, ST003D02T &lt;dbl&gt;,\n#   ST003D03T &lt;dbl&gt;, ST004D01T &lt;dbl&gt;, ST250Q01JA &lt;dbl&gt;, ST250Q02JA &lt;dbl&gt;,\n#   ST250Q03JA &lt;dbl&gt;, ST250Q04JA &lt;dbl&gt;, ST250Q05JA &lt;dbl&gt;, ST250D06JA &lt;chr&gt;,\n#   ST250D07JA &lt;chr&gt;, ST251Q01JA &lt;dbl&gt;, ST251Q02JA &lt;dbl&gt;, ST251Q03JA &lt;dbl&gt;, …"
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex02/Take-Home_Exercise02.html#data-wrangling",
    "href": "Take-home_Exercises/Take-Home_Ex02/Take-Home_Exercise02.html#data-wrangling",
    "title": "Viz- Makeover Exercise",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\nstu_qqq_SG_selectedV &lt;- stu_qqq_SG %&gt;% select(CNTSTUID, STRATUM, ST004D01T, IMMIG, ESCS,PV1READ:PV10READ, PV1SCIE:PV10SCIE, PV1MATH:PV10MATH)\nstu_qqq_SG_selectedV&lt;- stu_qqq_SG_selectedV %&gt;% mutate(CNTSTUID = as.character(CNTSTUID))\n\nnames(stu_qqq_SG_selectedV)[names(stu_qqq_SG_selectedV) == 'CNTSTUID'] &lt;- 'STUDENT ID'\n\nnames(stu_qqq_SG_selectedV)[names(stu_qqq_SG_selectedV) == 'ST004D01T'] &lt;- 'GENDER'\n\nstu_qqq_SG_selectedV &lt;- stu_qqq_SG_selectedV %&gt;%\n  mutate(GENDER = recode(as.character(GENDER), '1' = 'FEMALE', '2' = 'MALE'))\n\nstu_qqq_SG_selectedV &lt;- stu_qqq_SG_selectedV %&gt;%\n  mutate(STRATUM = recode(STRATUM, 'SGP01' = 'MAINSTREAM SCH', 'SGP03' = 'PRIVATE SCH'))\n\nstu_qqq_SG_selectedV&lt;- stu_qqq_SG_selectedV %&gt;%\n  mutate(IMMIG = recode(IMMIG, '1' = 'NATIVE', '2' = '2ND GEN', '3' = '1ST GEN'))"
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex02/Take-Home_Exercise02.html#initial-visualisation-outlook",
    "href": "Take-home_Exercises/Take-Home_Ex02/Take-Home_Exercise02.html#initial-visualisation-outlook",
    "title": "Viz- Makeover Exercise",
    "section": "3.1 Initial Visualisation Outlook",
    "text": "3.1 Initial Visualisation Outlook\nThe first example we look at a visualisation example of Average PV values across frequency below:\n\nIntended Viz’s Proposition\nIn this visualisation, the author created a Histogram with a Box plot to show the Average PVs versus Frequency across 3 subjects. It was a good attempt to show the readers the distribution skews, the statistical mean (with the red dot) and, possibly the outliers’ stretch to determine min|max range within each subjects."
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex02/Take-Home_Exercise02.html#potential-enhancements",
    "href": "Take-home_Exercises/Take-Home_Ex02/Take-Home_Exercise02.html#potential-enhancements",
    "title": "Viz- Makeover Exercise",
    "section": "3.2 Potential Enhancements",
    "text": "3.2 Potential Enhancements\nTo achieve the balance between aesthetics and clarity a few enhancements can be made:\n\nTo view the skewness of the distribution with accuracy, we must maintain the aspect ratio of the plots produced. This implies that patchwork function is not recommended as it will compress the aspect ratio making the scale of the axes inaccurate.\nThe X Axis should be represented accurate as the Average of PVs in each subject. while the Y Axis should be the score\nWe can also explore to include the Statistical Summary of Mean and Median values and the lines corresponding to it and input a legend on the top right for quick reference.\nBin width was selected at 20. This allows the balance of appeal (avoiding too many vertical lines) while still balancing the clarity of the shape of the distribution shown.\nThe Grid panels, major and minor lines were designed to be subtle-ly more apparent so that it can provide quick reference to x/y axis for the various bins\nBoth axes to start from ‘0’ to give an accurate representation of each bin.\nThe fill colour of the Histogram bins were made with high transparent fill effect, this is to allow easier reference of all the major and minor grid lines for x and y axes.\n\n\nMATHREADINGSCIENCE\n\n\n\n\nCode\nstu_qqq_SG_selectedV &lt;- stu_qqq_SG_selectedV %&gt;%\n  mutate(Maths = rowSums(stu_qqq_SG_selectedV[paste0('PV', c(1:10), \"MATH\")], \n                   na.rm = TRUE)/10) %&gt;% \n  mutate(Reading = \n           rowSums(stu_qqq_SG[paste0('PV', c(1:10), \"READ\")], \n                   na.rm = TRUE)/10) %&gt;% \n  mutate(Science = \n           rowSums(stu_qqq_SG[paste0('PV', c(1:10), \"SCIE\")], \n                   na.rm = TRUE)/10)\n\n# Maths Histogram and Boxplot with Mean and Median\nmaths_plot &lt;- ggplot(stu_qqq_SG_selectedV, aes(x = Maths)) +\n  geom_histogram(color = \"#459395\", binwidth = 20, fill = \"#459395\", alpha = 0.4) + coord_cartesian(xlim = c(0,1000), ylim = c(-80,600)) +\n  geom_boxplot(width = 40, position = position_nudge(y = -60), outlier.shape = 20, outlier.size = 4, outlier.color = \"darkblue\", fill = \"#459395\", color = \"#000000\", alpha = 0.1) +\n  geom_vline(aes(xintercept = mean(Maths, na.rm = TRUE)), linetype = \"dashed\", color = \"red\", size = 0.8, alpha = 0.5) +\n  annotate(\"text\", x = mean(stu_qqq_SG_selectedV$Maths, na.rm = TRUE) + 20, y = -20, label = paste(round(mean(stu_qqq_SG_selectedV$Maths, na.rm = TRUE), 2)), color = \"red\", size = 4, angle = 0, hjust = 1.35) +\n  geom_vline(aes(xintercept = median(Maths, na.rm = TRUE)), linetype = \"solid\", color = \"blue\", size = 0.8, alpha = 0.5) +\n  annotate(\"text\", x = median(stu_qqq_SG_selectedV$Maths, na.rm = TRUE) + 20, y = -20, label = paste(round(median(stu_qqq_SG_selectedV$Maths, na.rm = TRUE), 2)), color = \"blue\", size = 4, angle = 0, hjust = 0.15) +\n  labs(title = \"Distribution of Maths Scores (PV)\", x = \"Maths Scores\", y = \"Density\") +\n  theme_minimal()\n\nmaths_plot &lt;- maths_plot +\n  theme(panel.background = element_rect(fill = \"grey95\", color = NA),\n        panel.grid.major = element_line(color = \"grey80\", size = 0.5),\n        panel.grid.minor = element_line(color = \"grey90\", size = 0.25)) +\n  annotate(\"text\", x = 900, y = 580, label = \"Mean Value\", color = \"red\", size = 3.5, alpha = 0.8) +\n  annotate(\"text\", x = 900, y = 550, label = \"Median Value\", color = \"blue\", size = 3.5, alpha = 0.8)\n\nprint(maths_plot)\n\n\n\n\n\n\n\n\n\nCode\nreading_plot &lt;- ggplot(stu_qqq_SG_selectedV, aes(x = Reading)) +\n  geom_histogram(color = \"#EB7C69\", binwidth = 20, fill = \"#EB7C69\", alpha = 0.4) + coord_cartesian(xlim = c(0,1000), ylim = c(-80,600)) +\n  geom_boxplot(width = 40, position = position_nudge(y = -60), outlier.shape = 20, outlier.size = 4, outlier.color = \"red\", fill = \"#EB7C69\", color = \"#000000\", alpha = 0.1) +\n  geom_vline(aes(xintercept = mean(Reading, na.rm = TRUE)), linetype = \"dashed\", color = \"red\", size = 0.8, alpha = 0.5) +\n  annotate(\"text\", x = mean(stu_qqq_SG_selectedV$Reading, na.rm = TRUE) + 20, y = -20, label = paste(round(mean(stu_qqq_SG_selectedV$Reading, na.rm = TRUE), 2)), color = \"red\", size = 4, angle = 0, hjust = 1.35) +\n  geom_vline(aes(xintercept = median(Reading, na.rm = TRUE)), linetype = \"solid\", color = \"blue\", size = 0.8, alpha = 0.5) +\n  annotate(\"text\", x = median(stu_qqq_SG_selectedV$Reading, na.rm = TRUE) + 20, y = -20, label = paste(round(median(stu_qqq_SG_selectedV$Reading, na.rm = TRUE), 2)), color = \"blue\", size = 4, angle = 0, hjust = 0.15) +\n  labs(title = \"Distribution of Reading Scores (PV)\", x = \"Reading Scores\", y = \"Density\") +\n  theme_minimal()\n\nreading_plot &lt;- reading_plot +\n  theme(panel.background = element_rect(fill = \"grey95\", color = NA),\n        panel.grid.major = element_line(color = \"grey80\", size = 0.5),\n        panel.grid.minor = element_line(color = \"grey90\", size = 0.25)) +\n  annotate(\"text\", x = 900, y = 580, label = \"Mean Value\", color = \"red\", size = 3.5, alpha = 0.8) +\n  annotate(\"text\", x = 900, y = 550, label = \"Median Value\", color = \"blue\", size = 3.5, alpha = 0.8)\n\nprint(reading_plot)\n\n\n\n\n\n\n\n\n\nCode\nscience_plot &lt;- ggplot(stu_qqq_SG_selectedV, aes(x = Science)) +\n  geom_histogram(color = \"#FDA638\", binwidth = 20, fill = \"#FDA638\", alpha = 0.4) + coord_cartesian(xlim = c(0,1000), ylim = c(-80,600)) +\n  geom_boxplot(width = 40, position = position_nudge(y = -60), outlier.shape = 20, outlier.size = 4, outlier.color = \"darkorange\", fill = \"#FDA638\", color = \"#000000\", alpha = 0.1) +\n  geom_vline(aes(xintercept = mean(Science, na.rm = TRUE)), linetype = \"dashed\", color = \"red\", size = 0.8, alpha = 0.5) +\n  annotate(\"text\", x = mean(stu_qqq_SG_selectedV$Science, na.rm = TRUE) + 20, y = -20, label = paste(round(mean(stu_qqq_SG_selectedV$Science, na.rm = TRUE), 2)), color = \"red\", size = 4, angle = 0, hjust = 1.35) +\n  geom_vline(aes(xintercept = median(Science, na.rm = TRUE)), linetype = \"solid\", color = \"blue\", size = 0.8, alpha = 0.5) +\n  annotate(\"text\", x = median(stu_qqq_SG_selectedV$Science, na.rm = TRUE) + 20, y = -20, label = paste(round(median(stu_qqq_SG_selectedV$Science, na.rm = TRUE), 2)), color = \"blue\", size = 4, angle = 0, hjust = 0.15) +\n  labs(title = \"Distribution of Science Scores (PV)\", x = \"Science Scores\", y = \"Density\") +\n  theme_minimal()\n\nscience_plot &lt;- science_plot +\n  theme(panel.background = element_rect(fill = \"grey95\", color = NA),\n        panel.grid.major = element_line(color = \"grey80\", size = 0.5),\n        panel.grid.minor = element_line(color = \"grey90\", size = 0.25)) +\n  annotate(\"text\", x = 900, y = 580, label = \"Mean Value\", color = \"red\", size = 3.5, alpha = 0.8) +\n  annotate(\"text\", x = 900, y = 550, label = \"Median Value\", color = \"blue\", size = 3.5, alpha = 0.8)\n\nprint(science_plot)"
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex02/Take-Home_Exercise02.html#initial-intended-visualisation-outlook",
    "href": "Take-home_Exercises/Take-Home_Ex02/Take-Home_Exercise02.html#initial-intended-visualisation-outlook",
    "title": "Viz- Makeover Exercise",
    "section": "4.1 Initial Intended Visualisation Outlook",
    "text": "4.1 Initial Intended Visualisation Outlook\nNext, we look at a visualisation example of Distribution of Student Performances by Gender:\n\nIntended Viz’s Proposition\nIn this visualisation, the author created boxplots to show the Reading PVs across 3 subjects by Gender."
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex02/Take-Home_Exercise02.html#potential-enhancements-1",
    "href": "Take-home_Exercises/Take-Home_Ex02/Take-Home_Exercise02.html#potential-enhancements-1",
    "title": "Viz- Makeover Exercise",
    "section": "4.2 Potential Enhancements",
    "text": "4.2 Potential Enhancements\nTo achieve the balance between aesthetics and clarity these enhancements can be made:\n\nY axis must be accurately represented across all subjects. This will give a more coherent representation of the means and stats median across the 3 subjects.\nWe can also explore to include the Statistical Summary of Mean and Median values and the lines corresponding to it and input a legend on the top right for quick reference.\nA violin plot can be utilised in the background to give a sense of the distribution and range of the performance within each gender and subject domains.\nThe Grid panels, major and minor lines were designed to be subtle-ly more apparent so that it can provide quick reference to x/y axis for the various bins\nBoth axes to start from ‘0’ to give an accurate representation of each bin.\nThe fill colour of the Histogram bins were made with high transparent fill effect, this is to allow easier reference of all the major and minor grid lines for x and y axes.\n\n\n\nCode\nP7 &lt;- ggplot(data= stu_qqq_SG_selectedV,\n       aes(x= GENDER, y= Maths)) +\n  geom_violin(color = \"#459395\", size = 0.6, fill= \"#459395\", alpha = 0.4) +\n  geom_boxplot(width= 0.4, outlier.colour = \"grey30\", outlier.size = 3, outlier.color = \"darkblue\", \n               outlier.alpha = 0.5, outlier.shape = 19) +\n  stat_summary(geom = \"point\",       \n               fun =\"mean\",         \n               colour =\"red\",        \n               size=3) +  coord_cartesian(ylim = c(0,1000)) +\n  scale_color_manual(values=c(\"#999999\", \"#E69F00\")) +\n  theme_minimal() +\n  labs(title=\"Mathematics\") +  \n  theme(panel.background = element_rect(fill = \"grey95\", color = NA),\n        panel.grid.major = element_line(color = \"grey80\", size = 0.5),\n        panel.grid.minor = element_line(color = \"grey90\", size = 0.25),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        plot.title = element_text(size = 12, hjust = 0.5),\n        axis.text = element_text(size = 10))\nP7 &lt;- P7 +\n  stat_summary(aes(label = round(..y.., 2)), geom = \"text\", fun = mean, \n               vjust = -1.0, color = \"red\", size = 3) +\n  stat_summary(aes(label = round(..y.., 2)), geom = \"text\", fun = median, \n               vjust = 1.8, color = \"blue\", size = 3)\n  \n\nP8 &lt;- ggplot(data= stu_qqq_SG_selectedV,\n       aes(x= GENDER, y= Reading)) +\n  geom_violin(color = \"#EB7C69\", size = 0.6, fill= \"#EB7C69\", alpha = 0.4) +\n  geom_boxplot(width= 0.4, outlier.colour = \"grey30\", outlier.size = 3, outlier.color = \"darkred\", \n               outlier.alpha = 0.5, outlier.shape = 19) +\n  stat_summary(geom = \"point\",       \n               fun =\"mean\",         \n               colour =\"red\",        \n               size=3) +  coord_cartesian(ylim = c(0,1000)) +  \n  theme_minimal() +\n  labs(title=\"Reading\") + \n  theme(panel.background = element_rect(fill = \"grey95\", color = NA),\n        panel.grid.major = element_line(color = \"grey80\", size = 0.5),\n        panel.grid.minor = element_line(color = \"grey90\", size = 0.25),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        plot.title = element_text(size = 12, hjust = 0.5),\n        axis.text = element_text(size = 10))\n  \nP8 &lt;- P8 +\n  stat_summary(aes(label = round(..y.., 2)), geom = \"text\", fun = mean, \n               vjust = -1.0, color = \"red\", size = 3) +\n  stat_summary(aes(label = round(..y.., 2)), geom = \"text\", fun = median, \n               vjust = 1.8, color = \"blue\", size = 3)\n  \nP9 &lt;- ggplot(data= stu_qqq_SG_selectedV,\n       aes(x= GENDER, y= Science)) +\n  geom_violin(color = \"#FDA638\", size = 0.6, fill= \"#FDA638\", alpha = 0.4) +\n  geom_boxplot(width= 0.4, outlier.colour = \"grey30\", outlier.size = 3, outlier.color = \"red\", \n               outlier.alpha = 0.5, outlier.shape = 19) +\n  stat_summary(geom = \"point\",       \n               fun =\"mean\",         \n               colour =\"red\",        \n               size=3) +  coord_cartesian(ylim = c(0,1000)) +  \n  theme_minimal() +\n  labs(title=\"Science\") + \n  theme(panel.background = element_rect(fill = \"grey95\", color = NA),\n        panel.grid.major = element_line(color = \"grey80\", size = 0.5),\n        panel.grid.minor = element_line(color = \"grey90\", size = 0.25),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        plot.title = element_text(size = 12, hjust = 0.5),\n        axis.text = element_text(size = 10))\n\nP9 &lt;- P9 +\n  stat_summary(aes(label = round(..y.., 2)), geom = \"text\", fun = mean, \n               vjust = -1.0, color = \"red\", size = 3) +\n  stat_summary(aes(label = round(..y.., 2)), geom = \"text\", fun = median, \n               vjust = 1.8, color = \"blue\", size = 3) +\n  annotate(\"text\", x = 2.0, y = 1000, label = \"Mean Value\", color = \"red\", size = 3.5, alpha = 0.8) +\n  annotate(\"text\", x =2.0, y = 960, label = \"Median Value\", color = \"blue\", size = 3.5, alpha = 0.8)\n\n(P7 + P8 + P9) +\n    plot_annotation(title= \"Distribution of Performance (PV) by Gender\", subtitle = \"Higher mean scores in Math & Science, for Male Gender\", theme = theme(plot.title=element_text(size= 15, hjust= 0.5)))"
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex03/Take-home_Exercise03.html",
    "href": "Take-home_Exercises/Take-Home_Ex03/Take-home_Exercise03.html",
    "title": "Be Weatherwise or Otherwise",
    "section": "",
    "text": "Past climate trends over Singapore have shown an increase in surface air temperatures and the frequency of heavy rainfall over the past few decades. However, the climate system is complex and the past trends and the magnitude of the change will not necessarily continue into the future. Climate projections using tools like climate modelling is thus necessary to project the future climate for planning and adapting to climate change."
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex03/Take-home_Exercise03.html#developing-a-static-eda-on-daily-rainfall-by-day-and-year",
    "href": "Take-home_Exercises/Take-Home_Ex03/Take-home_Exercise03.html#developing-a-static-eda-on-daily-rainfall-by-day-and-year",
    "title": "Be Weatherwise or Otherwise",
    "section": "3.5.1 Developing a Static EDA on Daily Rainfall by Day and Year",
    "text": "3.5.1 Developing a Static EDA on Daily Rainfall by Day and Year\ngeom_point() function under the ggplot package was used to plot the total rainfall (mm) on a given day by year. In this static EDA, we can observe that:\n1) majority of the days by year, total rainfall (mm) was observed to be below 50mm while,\n2) in the years 1983 and 2013, where the total rainfall on day 25 and day 3 were above 100mm. We can infer that these were anomalies with significantly heavy rainfall as compared to the volumes of other rainy days by year.\n\n\n\n\n\n\nNote\n\n\n\nThe Static EDA has its shortcomings in terms of enhancing data appreciation. Next, we will move onto interactivity to enhance our data visualisation.\n\n\n\n\nCode\nlibrary(ggplot2)\n\nggplot(data = rainfall_table, aes(x = Day, y = `Daily Rainfall Total (mm)`, color = factor(Year))) +\n  geom_point() +  # Add points for the daily rainfall totals\n  theme_minimal() +  # Use a minimal theme\n  labs(title = \"Daily Rainfall Total (mm) by Day and Year\",\n       x = \"Day\",\n       y = \"Daily Rainfall Total (mm)\",\n       color = \"Year\") +  # Label the axes and the legend\n  scale_color_brewer(palette = \"Set1\")  # Use a color palette that's easily distinguishable"
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex03/Take-home_Exercise03.html#developing-interacitvity-using-plotly",
    "href": "Take-home_Exercises/Take-Home_Ex03/Take-home_Exercise03.html#developing-interacitvity-using-plotly",
    "title": "Be Weatherwise or Otherwise",
    "section": "3.5.2 Developing Interacitvity using plotly",
    "text": "3.5.2 Developing Interacitvity using plotly\nLeveraging on plotly the interactive scatter allows the reader to (1) hover for details (Day and Rainfall volume of the day) , (2) select (via cursor, legend or lasso) and (3) compare data on hover. For Sn (3), the function allows the reader to have a quick glance to make a quick analysis on the Daily Rainfall trends across the years. For instance, we can observe that:\n\n\nDay 1 and Day 2 records rainfalls on all 5 selected years, therefore we can infer that the probability of raining during the 1st two days of December will be higher (of course a larger data sample size will enhance the accuracy of this inference).\nDay 5, 7, 29 and 30 records the lowest rainfall over the 5 selected years, this can only be seen through the compare data on hover function."
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex03/Take-home_Exercise03.html#extending-plotly-further-to-capture-statistical-components",
    "href": "Take-home_Exercises/Take-Home_Ex03/Take-home_Exercise03.html#extending-plotly-further-to-capture-statistical-components",
    "title": "Be Weatherwise or Otherwise",
    "section": "3.5.3 Extending plotly further to capture Statistical Components",
    "text": "3.5.3 Extending plotly further to capture Statistical Components\nThe code chunk below, calculates the statistical summary of the scatter points which includes; (1) Total Rainfall by day, (2) Mean Rainfall by day and (3) Median Rainfall by day across all years.\n\n\nCode\nlibrary(dplyr)\n\n# Calculate summary statistics for each year\nsummary_stats_by_day &lt;- rainfall_table %&gt;%\n  group_by(Day) %&gt;%\n  summarise(\n    Total_Rainfall = sum(`Daily Rainfall Total (mm)`, na.rm = TRUE),\n    Mean_Rainfall = mean(`Daily Rainfall Total (mm)`, na.rm = TRUE),\n    Median_Rainfall = median(`Daily Rainfall Total (mm)`, na.rm = TRUE),\n  ) %&gt;%\n  ungroup()\n\n\n\n\nCode\nrainfall_table_with_stats &lt;- rainfall_table %&gt;%\n  left_join(summary_stats_by_day, by = \"Day\")\n\nlibrary(plotly)\n\nplot_ly(data = rainfall_table_with_stats, \n        x = ~Day, \n        y = ~`Daily Rainfall Total (mm)`,\n        color = ~factor(Year),\n        type = 'scatter',\n        mode = 'markers',\n        hoverinfo = 'text',\n        text = ~paste('Day:', Day,\n                      ', Year:', Year,\n                      '&lt;br&gt;Total Yearly Rainfall (mm):', Total_Rainfall,\n                      '&lt;br&gt;Mean Yearly Rainfall (mm):', Mean_Rainfall,\n                      '&lt;br&gt;Median Yearly Rainfall (mm):', Median_Rainfall)) %&gt;%\n  layout(title = 'Daily Rainfall Total (mm) with Sum_Stats by Day ',\n         xaxis = list(title = 'Day'),\n         yaxis = list(title = 'Rainfall (mm)'))\n\n\n\n\n\n\nHere with the stat summary, we can observe that individual points is supported by its stats summary of total, mean and median rainfall (mm) by day across the years."
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex03/Take-home_Exercise03.html#merging-temperature-records",
    "href": "Take-home_Exercises/Take-Home_Ex03/Take-home_Exercise03.html#merging-temperature-records",
    "title": "Be Weatherwise or Otherwise",
    "section": "3.6.1 Merging Temperature Records",
    "text": "3.6.1 Merging Temperature Records\n\n\nCode\ntemp_columns &lt;- c(\"Station\",\"Day\",\"Year\",\"Mean Temperature (°C)\", \"Maximum Temperature (°C)\", \"Minimum Temperature (°C)\")\n\ntemp_table &lt;- bind_rows(\n  select(data_1983, all_of(temp_columns))%&gt;% mutate(Year = as.character(Year)),\n  select(data_1993, all_of(temp_columns))%&gt;% mutate(Year = as.character(Year)),\n  select(data_2003, all_of(temp_columns))%&gt;% mutate(Year = as.character(Year)),\n  select(data_2013, all_of(temp_columns))%&gt;% mutate(Year = as.character(Year)),\n  select(data_2023, all_of(temp_columns))%&gt;% mutate(Year = as.character(Year)),\n)\n\n\nUtilising three key features of stat_halfeye(), geom_boxplot and geom_dotplot to study onto to the given data. The code chunk below shows the static plots:\n\n\nCode\nggplot(temp_table, aes(x = Year, y = `Mean Temperature (°C)`, fill = Year)) +\n  stat_halfeye(\n    adjust = 0.4, \n    position = position_nudge(x = 0.13), \n    color = NA, \n    alpha = 0.4\n  ) +\n  geom_boxplot(\n    width = 0.2, \n    outlier.shape = NA, \n    position = position_dodge(width = 0.5), \n    alpha = 0.6, \n    color = \"gray40\", \n    fill = \"grey93\"\n  ) +\n  geom_dotplot(\n    binaxis = \"y\", \n    stackdir = \"down\",\n    position = position_nudge(x = -0.13),\n    binwidth = 0.25, \n    dotsize = 0.25\n  ) +\n  scale_y_continuous(limits = c(23.5, 28.5)) +\n  scale_fill_brewer(palette = \"Pastel1\") +\n  theme_minimal(base_size = 8) +\n  theme(\n    legend.position = \"right\",\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.title = element_blank(),\n    plot.title = element_text(hjust = 0.5, size = 16),\n    plot.subtitle = element_text(hjust = 0.5, size = 12)\n  ) +\n  labs(\n    y = \"Mean Temperature (°C)\", \n    x = \"Year\", \n    title = \"Mean Temperature by Year\",\n    subtitle = \"Rising Mean Temperature over the decade\"\n  ) +\n  guides(fill = guide_legend(title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\n\nBased on the static EDA, we can see that the overall mean temperature for the selected year have increased across four decades. Through the `stat_halfeye’ it can be seen that (1) the range of fluctuating mean temps over the years has condensed significantly, (2) peaks (Frequencies of Higher Mean Temp) are compressed to be lesser but sharper in scale (i.e: 2023 has 1 main peak as compared to 1983 - 2003, with a higher peak) and (3) the number of dotplots (with binwidth=2.5) denotes the co-occurrence of a binned temperature of similar range over the given days."
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex03/Take-home_Exercise03.html#adding-interactivity-to-the-temp-scale",
    "href": "Take-home_Exercises/Take-Home_Ex03/Take-home_Exercise03.html#adding-interactivity-to-the-temp-scale",
    "title": "Be Weatherwise or Otherwise",
    "section": "3.6.2 Adding Interactivity to the Temp Scale",
    "text": "3.6.2 Adding Interactivity to the Temp Scale\nThrough the interactivity, the tooltip, provides precise interactivity to give the viewer more information on the points/dots denoted above. The codes below calculates other statistical components to be added back onto the temp_table.\n\n\nCode\n#| warning = FALSE\nlibrary(ggplot2)\nlibrary(ggiraph)\nlibrary(RColorBrewer)\nlibrary(dplyr)\n\nsummary_stats &lt;- temp_table %&gt;%\n  group_by(Year) %&gt;%\n  summarise(\n    OverallMean = mean(`Mean Temperature (°C)`, na.rm = TRUE),\n    OverallMin = min(`Mean Temperature (°C)`, na.rm = TRUE),\n    OverallMax = max(`Mean Temperature (°C)`, na.rm = TRUE),\n    Range = OverallMax - OverallMin  # Calculate the range\n  ) %&gt;%\n  ungroup()\n\ntemp_table_2 &lt;- temp_table %&gt;%\n  left_join(summary_stats, by = \"Year\")\n\nhead(temp_table_2)\n\n\n# A tibble: 6 × 10\n  Station   Day Year  `Mean Temperature (°C)` `Maximum Temperature (°C)`\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;                      &lt;dbl&gt;\n1 Changi      1 1983                     26.4                       31  \n2 Changi      2 1983                     24.3                       27.2\n3 Changi      3 1983                     25.1                       30.2\n4 Changi      4 1983                     25.2                       30.3\n5 Changi      5 1983                     26                         29.8\n6 Changi      6 1983                     25                         27.7\n# ℹ 5 more variables: `Minimum Temperature (°C)` &lt;dbl&gt;, OverallMean &lt;dbl&gt;,\n#   OverallMin &lt;dbl&gt;, OverallMax &lt;dbl&gt;, Range &lt;dbl&gt;\n\n\n\n\nCode\np &lt;- ggplot(temp_table_2, aes(x = Year, y = `Mean Temperature (°C)`, fill = Year)) +\n  stat_halfeye(\n    adjust = 0.4, \n    position = position_nudge(x = 0.2), \n    color = NA, \n    alpha = 0.4\n  ) +\n  geom_boxplot (\n    width = 0.2, \n    outlier.shape = NA, \n    position = position_dodge(width = 0.5), \n    alpha = 0.6, \n    color = \"gray40\", \n    fill = \"white\"\n  ) +\n  geom_point_interactive( \n    aes(\n      tooltip = paste('Day', `Day`, '&lt;br&gt;Mean Temp (°C):', `Mean Temperature (°C)`), \n      data_id = Year\n    ),\n    position = position_nudge(x = -0.2),\n    bindwidth = 2.5,\n    size = 1.3,  # Adjust the size as per your preference\n    alpha = 0.35\n  ) +\n  scale_y_continuous(limits = c(23.5, 28.5)) +\n  scale_fill_brewer(palette = \"Pastel1\") +\n  theme_minimal(base_size = 8) +\n  theme(\n    legend.position = \"right\",\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.title = element_blank(),\n    plot.title = element_text(hjust = 0.5, size = 16),\n    plot.subtitle = element_text(hjust = 0.5, size = 12)\n  ) +\n  labs(\n    y = \"Mean Temperature (°C)\", \n    x = \"Year\", \n    title = \"Mean Temperature by Year\",\n    subtitle = \"Rising Mean Temperature over the decade\"\n  ) +\n  guides(fill = guide_legend(title.position = \"top\", title.hjust = 0.5))\n\ng &lt;- girafe(                                  \n ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n   opts_hover(css = \"fill: #963000;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n ) \ng\n\n\n\n\n\n\n\n\nThrough the interactivity on the geom_boxplot, it can be observed that the Overall Mean and Max temperature has increased over the years with the year 2023 having the highest temperature.\nIt can also be observed that the range is much more condensed in the Year 2023, with that we can infer that as the Max Temp rises the Minimum temperature over the years have risen as well.\nThrough the intensity of the dotplot and the lumpiness of the halfeye, it can be seen that most 2023 has a ‘single’ condensed region unlike other years where there are mulitple peaks."
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex04/Take-Home_Exercise04.html",
    "href": "Take-home_Exercises/Take-Home_Ex04/Take-Home_Exercise04.html",
    "title": "Big Mac Index Choropleth and Time Series",
    "section": "",
    "text": "pacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               ggdist, ggridges,\n               colorspace, gridExtra,\n               tidyverse, tmap, sf, tmaptools, dplyr, tibble, dplyr) \n\nImporting data\n\nbig_mac &lt;- read_csv(\"data/countries_with_complete_data.csv\")"
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex04/Take-Home_Exercise04.html#calling-out-forecast-packages",
    "href": "Take-home_Exercises/Take-Home_Ex04/Take-Home_Exercise04.html#calling-out-forecast-packages",
    "title": "Big Mac Index Choropleth and Time Series",
    "section": "3.1 Calling out Forecast Packages",
    "text": "3.1 Calling out Forecast Packages\n\nlibrary(forecast)\nlibrary(tibble)\nlibrary(dplyr)"
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex04/Take-Home_Exercise04.html#filter-data-to-time-series-object",
    "href": "Take-home_Exercises/Take-Home_Ex04/Take-Home_Exercise04.html#filter-data-to-time-series-object",
    "title": "Big Mac Index Choropleth and Time Series",
    "section": "3.2 Filter Data to Time Series Object",
    "text": "3.2 Filter Data to Time Series Object\n\nuk_data &lt;- big_mac %&gt;%\n  filter(country == \"United Kingdom\", year &gt;= 2002, year &lt;= 2022) %&gt;%\n  arrange(year) %&gt;%\n  select(year, bmi_usd_price)\n\n# Convert to ts object\nuk_ts &lt;- ts(uk_data$bmi_usd_price, start = 2002, end = 2021, frequency = 2)"
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex04/Take-Home_Exercise04.html#model-fitting-using-auto.arima",
    "href": "Take-home_Exercises/Take-Home_Ex04/Take-Home_Exercise04.html#model-fitting-using-auto.arima",
    "title": "Big Mac Index Choropleth and Time Series",
    "section": "3.3 Model Fitting using auto.arima",
    "text": "3.3 Model Fitting using auto.arima\nUse the auto.arima function to automatically select the best ARIMA model based on AIC or BIC.\n\nLower AIC or BIC values indicate a better-fitting model.\nAIC focuses on the goodness of fit but can suffer from overfitting.\nBIC adds a penalty term for the number of parameters in the model, which can result in choosing simpler models than AIC.\n\n\nlibrary(forecast)\n\n# The auto.arima function will search through different ARIMA models \nfit &lt;- auto.arima(uk_ts)\n\n# the summary of the fitted model which includes the AIC and BIC values\nsummary(fit)\n\nSeries: uk_ts \nARIMA(1,0,0) with non-zero mean \n\nCoefficients:\n         ar1    mean\n      0.5282  3.8809\ns.e.  0.1448  0.1399\n\nsigma^2 = 0.189:  log likelihood = -21.98\nAIC=49.97   AICc=50.65   BIC=54.96\n\nTraining set error measures:\n                     ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.01733105 0.4233925 0.3306332 -0.807335 8.747498 0.7345117\n                    ACF1\nTraining set -0.03813155\n\n# compare AIC and BIC manually if needed\naic_value &lt;- fit$aic\nbic_value &lt;- fit$bic\n\n# Print the AIC and BIC values\ncat(\"AIC:\", aic_value, \"\\n\")\n\nAIC: 49.96672 \n\ncat(\"BIC:\", bic_value, \"\\n\")\n\nBIC: 54.9574 \n\nfit_bic &lt;- auto.arima(uk_ts, ic = \"bic\")\n\n# Print summary of the model selected based on BIC\nsummary(fit_bic)\n\nSeries: uk_ts \nARIMA(1,0,0) with non-zero mean \n\nCoefficients:\n         ar1    mean\n      0.5282  3.8809\ns.e.  0.1448  0.1399\n\nsigma^2 = 0.189:  log likelihood = -21.98\nAIC=49.97   AICc=50.65   BIC=54.96\n\nTraining set error measures:\n                     ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.01733105 0.4233925 0.3306332 -0.807335 8.747498 0.7345117\n                    ACF1\nTraining set -0.03813155\n\n\nACF and PACF plots identify whether an AR, MA, or ARMA model will be more appropriate:\nAR (Autoregressive) model: The Partial Autocorrelation Function (PACF) plot would show a significant spike at the lag corresponding to the order of the AR term (p), and then it would cut off, meaning other spikes should not be significant. The Autocorrelation Function (ACF) plot would show a more gradual decline.\nMA (Moving Average) model: The Autocorrelation Function (ACF) plot would show a significant spike at the lag corresponding to the order of the MA term (q), and then it would cut off. The Partial Autocorrelation Function (PACF) would show a more gradual decline.\nARMA (Autoregressive Moving Average) model: Both ACF and PACF plots show a more complex pattern, with neither cutting off abruptly, indicating a mixture of AR and MA behaviours.\nTo determine the specific AR or MA orders, it would typically look for the number of lags before the plot crosses the significance boundary (the blue dashed lines). Spikes that go beyond this boundary are considered significant.\nWhen you’re looking at your ACF and PACF plots, consider the following:\nIf there are a few significant spikes in the PACF followed by non-significant ones, and the ACF tails off, consider an AR model with the order determined by the number of significant spikes. If there are a few significant spikes in the ACF followed by non-significant ones, and the PACF tails off, consider an MA model with the order determined by the number of significant spikes. If both the ACF and PACF show a complex pattern without a clear cutoff, an ARMA model might be needed, where it would need to experiment with different orders to find the best fit.\n\n# Load the necessary library\nlibrary(forecast)\n\n# Assuming uk_ts is your time series object\n# Plot the ACF\nacf(uk_ts)\n\n\n\n# Plot the PACF\npacf(uk_ts)"
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex04/Take-Home_Exercise04.html#forecasting-and-plotting",
    "href": "Take-home_Exercises/Take-Home_Ex04/Take-Home_Exercise04.html#forecasting-and-plotting",
    "title": "Big Mac Index Choropleth and Time Series",
    "section": "3.4 Forecasting and plotting",
    "text": "3.4 Forecasting and plotting\nForecasting the bmi_usd_price up to 2030\n\n# Load the necessary libraries\nlibrary(forecast)\n\n# Read in your time series data\nuk_ts &lt;- ts(uk_data$bmi_usd_price, start = 2002, end = 2021, frequency = 1)\n\n# Fit an ARIMA model using auto.arima\nauto_fit &lt;- auto.arima(uk_ts)\n\n# Forecast from 2021 to 2030\nforecast_length &lt;- 2030 - max(time(uk_ts))\nforecast_result &lt;- forecast(auto_fit, h=forecast_length)\n\n# Plot the forecast with actual data\nplot(forecast_result)\nlines(uk_ts, type=\"o\", col='blue')\n\n\n\n\nIn the context of ARIMA models, the order argument specifies the order of the model as a triplet of parameters (p, d, q):\n\np: The number of autoregressive (AR) terms. It represents the number of lags of the dependent variable that are used as predictors. For instance, if p is 3, the predictors for x(t) will be x(t−1),x(t−2), and x(t−3).\nd: The degree of differencing. It indicates the number of times the data have had past values subtracted. Differencing is used to make the series stationary, which is a requirement for many statistical modelling methods.\nq: The number of moving average (MA) terms. This parameter is associated with the number of lagged forecast errors in the prediction equation. For example, if q is 2, the prediction of x(t) will be adjusted by the forecast errors made on the two previous predictions x(t−1) and x(t−2).\n\nSo when order=c(1,1,1) is in use, it specifies an ARIMA model with:\n\n1 AR term (p=1)\n1 degree of differencing (d=1)\n1 MA term (q=1)\n\nThe choice of these parameters is critical as they define the structure of the time series model you are trying to fit. The ARIMA model attempts to describe the autocorrelations in the data with these parameters.\n\n# If you want to manually specify the model, fit it like so:\nmanual_fit &lt;- Arima(uk_ts, order=c(2,1,8))\n\n\n\n# And forecast manually specified model from 2021 to 2030\nmanual_forecast &lt;- forecast(manual_fit, h=forecast_length, level=c(90, 95))\n\n# Plot the manual forecast\nplot(manual_forecast)\nlines(uk_ts, type=\"o\", col='red') \n\n\n\n\nAdding a slope/gradient to the forecast and the Confidence Interval L: 90% + U:95%\n\n# Fit a linear model to your time series data\ntrend_model &lt;- lm(uk_ts ~ time(uk_ts))\n\n# If you want to manually specify the model, fit it like so:\nmanual_fit &lt;- Arima(uk_ts, order=c(2,1,8))\n\n# And forecast manually specified model from 2021 to 2030\nmanual_forecast &lt;- forecast(manual_fit, h=forecast_length, level=c(90, 95))\n\nlower_90 &lt;- manual_forecast$lower[,1]  # 90% confidence interval lower bound\nupper_90 &lt;- manual_forecast$upper[,1]  # 90% confidence interval upper bound\nlower_95 &lt;- manual_forecast$lower[,2]  # 95% confidence interval lower bound\nupper_95 &lt;- manual_forecast$upper[,2]  # 95% confidence interval upper bound\n\n# To display these values, you could use a data frame\nconfidence_intervals &lt;- data.frame(\n  Time = time(manual_forecast$mean),\n  Lower_90 = lower_90,\n  Upper_90 = upper_90,\n  Lower_95 = lower_95,\n  Upper_95 = upper_95\n)\n\nprint(confidence_intervals)\n\n  Time Lower_90 Upper_90 Lower_95 Upper_95\n1 2022 3.555345 5.040414 3.413095 5.182663\n2 2023 3.362534 5.071375 3.198849 5.235059\n3 2024 3.587076 5.327887 3.420329 5.494634\n4 2025 3.281339 5.185129 3.098981 5.367487\n5 2026 3.522855 5.569848 3.326780 5.765922\n6 2027 3.273329 5.470929 3.062828 5.681430\n7 2028 3.238782 5.914370 2.982496 6.170656\n8 2029 2.990264 5.940387 2.707681 6.222970\n9 2030 2.954290 6.284792 2.635272 6.603810\n\n# Plot the manual forecast\nplot(manual_forecast, main = \"UK's Forecasted BMI value with CI\",  # Add a title\n     xlab = \"Year\",  # Label for the x-axis\n     ylab = \"BMI_USDprice\",  # Label for the y-axis\n     col = \"blue\",  # Color for the forecast line\n     type = \"n\",\n     lwd = 1,  # Width of the forecast line\n     cex.lab = 0.8,  # Size of axis labels\n     cex.axis = 0.8,  # Size of axis tick labels\n     cex.main = 1) \nlines(manual_forecast$mean, col = \"blue\", lwd = 2)\npoints(uk_ts, pch = 19, col = \"red\")\nlines(uk_ts, type=\"o\", col='red')\n\n# Add a linear trend line to the plot\nabline(trend_model$coefficients, col=\"grey\")\nlegend(\"topleft\", legend = c(\"Forecast\", \"Historical Data\", \"Trend Line\"), col = c(\"blue\", \"red\", \"darkgrey\"), lty = 1, lwd = 0.5, pch = c(NA, 19, NA))\n\n# Get the slope (gradient) of the linear model\nslope &lt;- coef(trend_model)[\"time(uk_ts)\"]\n\n# Add the slope value as text on the plot\n# You can adjust the x and y coordinates and the text size (cex) as needed\ntext(x = mean(time(uk_ts)), y = max(uk_ts), labels = paste(\"Gradient:\", round(slope, 3)), pos = 3, cex = 0.8)\n\nx_offset &lt;- 16\n\nci_x &lt;- c(time(manual_forecast$mean), rev(time(manual_forecast$mean)))\nci_y &lt;- c(manual_forecast$lower[,2], rev(manual_forecast$upper[,2]))\npolygon(ci_x, ci_y, col = rgb(0, 1, 1, alpha = 0.1), border = NA)\n\n\n\n\n\nlibrary(plotly)\n# Assuming manual_forecast, trend_model, and uk_ts are already defined\n\n# Convert uk_ts to a data frame for plotly (if it's not already)\nuk_ts_df &lt;- data.frame(Time = as.numeric(time(uk_ts)), Value = as.numeric(uk_ts))\n\n# Create base plot with plotly\np &lt;- plot_ly() %&gt;%\n  add_lines(data = uk_ts_df, x = ~Time, y = ~Value, name = \"Historical Data\", line = list(color = 'red')) %&gt;%\n  add_lines(x = time(manual_forecast$mean), y = manual_forecast$mean, name = \"Forecast\", line = list(color = 'blue')) %&gt;%\n  add_ribbons(x = time(manual_forecast$mean), ymin = manual_forecast$lower[,2], ymax = manual_forecast$upper[,2], name=\"95% CI\", line = list(color = 'transparent'), fillcolor = 'rgba(0, 0, 255, 0.1)') %&gt;%\n  layout(title = \"UK's Forecasted BMI value with CI\", xaxis = list(title = \"Year\"), yaxis = list(title = \"BMI_USDprice\"))\n\n# Add trend line - calculate points based on the linear model coefficients\ntrend_line_df &lt;- data.frame(Time = c(min(uk_ts_df$Time), max(uk_ts_df$Time)))\ntrend_line_df$Trend &lt;- coef(trend_model)[\"(Intercept)\"] + coef(trend_model)[\"time(uk_ts)\"] * trend_line_df$Time\n\n# Add trend line to the plot\np &lt;- p %&gt;% add_lines(data = trend_line_df, x = ~Time, y = ~Trend, name = \"Trend Line\", line = list(color = 'grey'))\n\n# Optionally, add annotations for the slope and CI labels\n# Note: Adjust the coordinates (x, y) based on your data's range\np &lt;- p %&gt;%\n  layout(annotations = list(\n    list(x = mean(uk_ts_df$Time), y = max(uk_ts_df$Value), text = paste(\"Gradient:\", round(slope, 3)), showarrow = F),\n    list(x = max(uk_ts_df$Time), y = min(manual_forecast$lower[length(manual_forecast$mean),2]), text = paste(\"Lower 95%: Mean\", round(manual_forecast$lower[length(manual_forecast$mean),2], 2)), showarrow = F),\n    list(x = max(uk_ts_df$Time), y = max(manual_forecast$upper[length(manual_forecast$mean),2]), text = paste(\"Upper 95%: Mean\", round(manual_forecast$upper[length(manual_forecast$mean),2], 2)), showarrow = F)\n  ))\n\n# Render the plot\np"
  },
  {
    "objectID": "Take-home_Exercises/Take-Home_Ex04/Take-Home_Exercise04.html#residual-testing",
    "href": "Take-home_Exercises/Take-Home_Ex04/Take-Home_Exercise04.html#residual-testing",
    "title": "Big Mac Index Choropleth and Time Series",
    "section": "3.5 Residual Testing",
    "text": "3.5 Residual Testing\n\n3.5.1 Checking Normality through Shapiro.Test\n\n# Assuming 'fit' is your fitted ARIMA model\nresiduals &lt;- residuals(manual_fit)\n\n# Plot histogram\nhist(residuals, breaks = 30, main = \"Histogram and Density Plot of Residuals\", xlab = \"Residuals\", col = \"skyblue\", border = \"white\", probability = TRUE)\n\n# Overlay density plot\nlines(density(residuals), col = \"red\", lwd = 2)\n\n\n\n\n\n# Shapiro-Wilk normality test\nshapiro.test(residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals\nW = 0.97558, p-value = 0.8654\n\n\nW: This is the test statistic value, where W ranges from 0 to 1. A value of 1 indicates perfect normality.\np-value = 0.8108: A common threshold for significance is 0.05. If the p-value is less than 0.05, we reject the null hypothesis and conclude that the data do not come from a normally distributed population. If the p-value is greater than 0.05.\nIn this result, with a p-value of 0.8108, there is strong evidence to suggest that residuals are normally distributed.\n\n\n3.5.2 Using QQ-plot\nA Q-Q (quantile-quantile) plot compares the distribution of residuals to a normal distribution.\n\nlibrary(car)\n\n# Create Q-Q plot with statistical annotations\nqqPlot(residuals(manual_fit), main=\"Q-Q Plot of Residuals\", \n       ylab=\"Sample Quantiles\", \n       las=1)\n\n\n\n\n[1] 7 8\n\n\n\n\n3.5.3 Checking for autocorrelation at any given lags\nThe Ljung-Box test provides a p-value that can be used to determine whether there is significant evidence of autocorrelation at any of the lags tested.\n\n# Perform the Ljung-Box test and store the result\nlb_test &lt;- Box.test(residuals(fit), type=\"Ljung-Box\")\n\n# Print the test statistic and p-value\ncat(\"Ljung-Box test statistic:\", lb_test$statistic, \"\\n\")\n\nLjung-Box test statistic: 0.06118341 \n\ncat(\"P-value:\", lb_test$p.value, \"\\n\")\n\nP-value: 0.8046352 \n\n\nIn the context of the Ljung-Box test, a higher p-value is generally considered better when using it to check the residuals of a time series model for randomness or lack of autocorrelation. The Ljung-Box test is a type of statistical test that is used to determine if there are significant autocorrelations at lag in the residuals of a time series model.\n\nHigher p-value (typically &gt; 0.05): Suggests that the residuals are random, indicating that the model has adequately captured the information in the data. In other words, there is no evidence of significant autocorrelation at any of the tested lags, and the model fits well.\nLower p-value (typically ≤ 0.05): Suggests that there is significant autocorrelation in the residuals that the model has not captured. This can indicate that the model could be improved, either by adding more terms or by considering a different model structure.\n\nTherefore, we can observe that the p-value is above 0.138 and hence we can reject the null hypothesis that there is a autocorrelation. This outcome would suggest that our model is capturing the underlying patterns in the data well, and the residuals are essentially white noise, as desired.\n\n# Plotting ACF of residuals\nAcf(residuals(manual_fit), main='ACF of Residuals')\n\n\n\n# Conducting the Ljung-Box test\nBox.test(residuals(manual_fit), lag = log(length(residuals(fit))))\n\n\n    Box-Pierce test\n\ndata:  residuals(manual_fit)\nX-squared = 0.32999, df = 3.6636, p-value = 0.9807\n\n\nFor ACF plot of residuals, the goal is to see no pattern and have all bars within the confidence interval, indicating that the residuals are random (white noise) and the model has done a good job of capturing the underlying patterns in the data.\nIf there is significant autocorrelations, there is a need to revise the model by adjusting its parameters or by considering additional explanatory variables.\n\nFor the purpose of Shiny App, all Countries will run ARIMA with the option for the viewer to select the (1) model fit criteria, (2) parameters and (3) residual checks to see if the model suffers from overfitting or autocorellation."
  },
  {
    "objectID": "index.html#three-stages-of-a-visual-analytics-student",
    "href": "index.html#three-stages-of-a-visual-analytics-student",
    "title": "ISS608",
    "section": "Three Stages of a Visual Analytics Student",
    "text": "Three Stages of a Visual Analytics Student\nMy journey through attaining data visualisation knowledge.\n\n\n\n\n\n\n\n\nStage 1: Highly Diligent & Optimistic\nStage 2: On Overdrive & Caffeine IV Drip\nStage 3: Questioning Life Choices\n\n\n\n\n\n\n\n\n\n\n\nHighly Diligent and Optimistic: This stage is all about the initial excitement and motivation. Students are full of energy, ready to conquer every textbook and lecture note. They’re the ones sitting in the front row, taking meticulous notes and perhaps even colour-coding them for good measure.\nOn Overdrive & Caffeine IV Drip: At this point, the workload has increased, and so has the reliance on coffee or any form of caffeine. Nights become longer, sleep becomes a distant memory, and the library feels more like home than the actual dorm. The motto shifts to “I’ll sleep when I’m dead” as deadlines loom closer.\nQuestioning Life Choices: This final stage is marked by a deep reevaluation of one’s life decisions leading up to this point. The workload feels insurmountable, leading to moments of existential dread and questioning whether the subject of study is a passion or a punishment. Acceptance of one’s fate becomes the coping mechanism, along with a dark sense of humour about the future."
  },
  {
    "objectID": "index.html#the-workload",
    "href": "index.html#the-workload",
    "title": "ISS608",
    "section": "The Workload",
    "text": "The Workload"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About FirGhaz",
    "section": "",
    "text": "The Dilemma\nThat was me at the start of the ADD | DROP period. Ouh well, Hi *Firdaus* here! Thrilled to be on this VAA journey, under the stewardship of Prof Kam! 😅"
  }
]